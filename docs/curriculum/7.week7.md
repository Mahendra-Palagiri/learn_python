> [ðŸ”¼ README](../../README.md)

### Week 7 â€” Day-by-Day Breakdown

## Week 7 Theme: Model Selection & Validation âš–ï¸

**Goal (by end of Week 7):**

- Understand the **biasâ€“variance tradeoff** (intuition + observable symptoms)
- Use **cross-validation correctly** (including the â€œmath ideaâ€ behind why it estimates generalization)
- Build a **leakage-safe model selection workflow** using Pipelines + CV + tuning
- Compare models with **evidence** (metric choice, stability, uncertainty) and defend the final pick

## Week 7 â€” Day 1 âœ…  

- ***Focus: Biasâ€“Variance Tradeoff (Symptoms + Diagnosis)***
- Defined bias vs variance in practical terms:
  - **High bias** â†’ underfit (too simple), poor training + validation performance
  - **High variance** â†’ overfit (too flexible), good training but poor validation performance
- Learned how model complexity shifts bias/variance:
  - linear vs polynomial degree
  - regularization strength (Ridge/Lasso)
  - tree depth / min samples
- Used **learning curves** (train vs validation error vs sample size) to diagnose:
  - data scarcity vs model mismatch vs noise ceiling

âœ… **Outcome:** You can look at train/val behavior and explain whether youâ€™re bias-limited or variance-limited.

**Checkpoint Questions:**

- If training error is high and validation error is also high, whatâ€™s the first fix you try?
- If training error is low but validation error is high, what class of fixes usually help?


## Week 7 â€” Day 2 âœ…  

- ***Focus: Model Assessment vs Model Selection + Splitting Patterns***
- Clarified the difference:
  - **Model selection** = choosing model + hyperparameters (tune)
  - **Model assessment** = final performance estimate (report)
- Practiced correct split strategies:
  - Train/Validation/Test (basic)
  - CV for tuning + Holdout Test for final reporting
- Learned what **data leakage** is and common ways it sneaks in:
  - scaling/encoding before split
  - target leakage via â€œfutureâ€ fields
  - feature selection using full dataset
  - time-series shuffling

âœ… **Outcome:** You can design a splitting strategy that supports both tuning and honest evaluation.

**Checkpoint Questions:**

- Why is it invalid to tune hyperparameters on the test set â€œjust onceâ€?
- Name 2 leakage examples that would inflate validation performance.


## Week 7 â€” Day 3 âœ…  

- ***Focus: Cross-Validation Mechanics + CV Math (Light but Real)***
- Implemented k-fold cross-validation and understood what it does:
  - produces a **distribution of scores** (not just one number)
  - reduces dependence on a single random split
- Understood why k-fold estimates generalization:
  - each point serves as validation exactly once (in standard k-fold)
  - averaged score approximates expected out-of-sample performance
- Built intuition for **uncertainty**:
  - mean CV score vs standard deviation across folds
  - what â€œunstable modelâ€ looks like in fold variance

âœ… **Outcome:** You can explain CV as a repeated â€œtrain on most / test on held-outâ€ experiment and interpret mean vs variance.

**Checkpoint Questions:**

- What changes when k is very small vs very large?
- Why is â€œCV meanâ€ not the same thing as â€œtrue test performanceâ€?


## Week 7 â€” Day 4 âœ…  

- ***Focus: Pipelines + Preprocessing INSIDE CV (Leakage-Proof)***
- Built sklearn `Pipeline` for end-to-end training:
  - preprocessing + model fitted *within each fold*
- Used `ColumnTransformer` for mixed data:
  - StandardScaler for numeric
  - OneHotEncoder for categorical
- Learned why Pipelines matter for truth:
  - prevents leakage from scaling/encoding using validation fold statistics

âœ… **Outcome:** You can build a reusable, leakage-safe pipeline that works with CV and tuning.

**Checkpoint Questions:**

- What exactly leaks if you scale the whole dataset before CV?
- Why does one-hot encoding need to be inside the CV loop too?


## Week 7 â€” Day 5 âœ…  

- ***Focus: Hyperparameter Tuning with CV (Grid vs Random)***
- Used CV-based tuning correctly:
  - `GridSearchCV` for small, curated search spaces
  - `RandomizedSearchCV` for wider ranges and efficiency
- Learned practical tuning rules:
  - start with a baseline
  - tune a few meaningful knobs
  - evaluate improvements with CV distribution (mean + variance)
- Captured results:
  - best params
  - CV score mean/stdev
  - top-N configurations

âœ… **Outcome:** You can tune hyperparameters using CV without fooling yourself.

**Checkpoint Questions:**

- When is RandomizedSearch better than GridSearch?
- Why does â€œbest CV scoreâ€ still need a final test evaluation?


## Week 7 â€” Day 6 âœ…  

- ***Focus: Model Comparison + Nested CV (Honest Selection)***
- Compared multiple candidate models with the same evaluation protocol:
  - baseline vs candidate models
  - consistent preprocessing via Pipelines
- Learned about **nested CV**:
  - inner CV = tuning
  - outer CV = unbiased performance estimate of the tuned procedure
- Worked with metric selection:
  - classification: accuracy vs F1 vs ROC-AUC vs PR-AUC (imbalance awareness)
  - regression: MAE vs RMSE (error sensitivity)
- Interpreted stability:
  - high mean + low variance (preferred)
  - high mean + high variance (risky)

âœ… **Outcome:** You can compare models fairly and explain why nested CV is needed for honest â€œbest modelâ€ claims.

**Checkpoint Questions:**

- What problem does nested CV solve that regular CV tuning doesnâ€™t?
- If Model A has slightly lower mean but much lower variance than Model B, which might you pick and why?


## Week 7 â€” Day 7 âœ…  

- ***Capstone: Model Selection & Validation Report (Evidence-Based Pick)***
- Built an end-to-end model selection workflow:
  1) Define baseline + metric
  2) Build Pipeline(s)
  3) Tune with CV (Grid/Random)
  4) Compare candidates (CV mean + variance)
  5) (Optional) Nested CV estimate for the whole selection process
  6) Final holdout test evaluation (one-time)
  7) Write a concise â€œModel Defenseâ€:
     - metric choice and why
     - evidence from CV distribution
     - stability/uncertainty discussion
     - leakage checks performed
     - risks / failure modes to monitor

âœ… **Outcome:** You can defend a final model choice using a clean validation design and CV evidence.

---

## Week 7 --> Explained in Simple words 

- ***Week 7 is where we stop â€œtraining modelsâ€ and start training our judgment***

- ***What weâ€™ll do (in simple words):***
  - Learn how to choose the right model instead of just picking the one that looks good on one split.
  - Understand underfitting vs overfitting (bias vs variance) so we know why a model is failing.
  - Use cross-validation (multiple train/validate rounds) to get a more reliable performance estimate.
  - Tune settings (hyperparameters) the right way using pipelines so we donâ€™t accidentally â€œcheatâ€ with data leakage.

- ***Why weâ€™re doing it:***
	- Because a model that looks great on one split can be a lucky accident.
	- We want performance that holds up on new, unseen dataâ€”thatâ€™s the whole point of ML.

- ***How it helps our AI journey:***
	- This turns us from â€œsomeone who can fit modelsâ€ into â€œsomeone who can ship models.â€
	- Weâ€™ll be able to defend a model choice with evidence, avoid common traps, and build workflows that work in real projects.


---

### Core Libraries and Setup

```bash
source .venv/bin/activate
pip install -U pip
pip install numpy pandas matplotlib scikit-learn scipy
pip freeze > requirements.txt
```


---

## Key Skills

### Model Selection & Validation

- Model selection vs model assessment
- Train/validation/test vs CV-based selection
- Leakage detection and prevention

### Biasâ€“Variance Thinking
- Underfit vs overfit diagnosis
- Learning curves for diagnosis
- Complexity control (regularization, model capacity)

### Cross-Validation
- k-fold, stratified k-fold, repeated CV
- CV mean vs variance (stability)
- Nested CV for unbiased selection reporting

### Tuning & Comparison
- GridSearchCV vs RandomizedSearchCV
- Pipelines + ColumnTransformer (preprocessing inside CV)
- Metric choice and interpretation
  
---

### Examples

#### Example 1 â€” Learning Curve (Bias/Variance Diagnosis)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

pipe = Pipeline([
    ("scale", StandardScaler()),
    ("model", Ridge(alpha=1.0))
])

train_sizes, train_scores, val_scores = learning_curve(
    pipe, X, y,
    train_sizes=np.linspace(0.1, 1.0, 8),
    cv=5,
    scoring="neg_mean_squared_error",
    random_state=42
)

train_err = -train_scores.mean(axis=1)
val_err = -val_scores.mean(axis=1)

plt.plot(train_sizes, train_err, label="Train error")
plt.plot(train_sizes, val_err, label="CV error")
plt.xlabel("Training set size")
plt.ylabel("MSE")
plt.legend()
plt.show()
```

#### Example 2 - CV Scores as a Distribution (Stability)

```python
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

pipe = Pipeline([
    ("scale", StandardScaler()),
    ("model", LogisticRegression(max_iter=500))
])

scores = cross_val_score(pipe, X, y, cv=5, scoring="f1")
print("F1 mean:", scores.mean())
print("F1 std :", scores.std())
print("Fold scores:", scores)
```

#### Example 3 â€” GridSearch with Pipeline (No Leakage)

```python
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

pipe = Pipeline([
    ("scale", StandardScaler()),
    ("model", SVC())
])

param_grid = {
    "model__C": [0.1, 1, 10],
    "model__gamma": ["scale", 0.01, 0.1],
    "model__kernel": ["rbf"]
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

gs = GridSearchCV(pipe, param_grid=param_grid, cv=cv, scoring="f1", n_jobs=-1)
gs.fit(X_train, y_train)

print("Best params:", gs.best_params_)
print("Best CV score:", gs.best_score_)
print("Test score:", gs.score(X_test, y_test))
```

#### Example 4 â€” Nested CV Skeleton (Honest Estimate for Tuning Process)

```python
import numpy as np
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

pipe = Pipeline([
    ("scale", StandardScaler()),
    ("model", LogisticRegression(max_iter=500))
])

param_grid = {
    "model__C": [0.1, 1, 10],
    "model__penalty": ["l2"]
}

inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)

search = GridSearchCV(pipe, param_grid=param_grid, cv=inner_cv, scoring="f1", n_jobs=-1)
nested_scores = cross_val_score(search, X, y, cv=outer_cv, scoring="f1")

print("Nested CV F1 mean:", nested_scores.mean())
print("Nested CV F1 std :", nested_scores.std())
```