> [ðŸ”¼ README](../../README.md)

### Week 5 â€” Day-by-Day Breakdown

## Week 5 â€” Day 1 âœ…  
- ***Focus: Logistic Regression Fundamentals***
- Understood what classification means vs regression.
- Learned the logistic (sigmoid) function and how it maps real numbers â†’ probabilities.
- Explored odds, log-odds, and why logistic regression is a linear model in log-odds space.
- Implemented the sigmoid manually and validated output behavior.
- Trained your first Logistic Regression baseline on Titanic dataset.  

âœ… **Outcome:** Strong intuition for how logistic regression works and why it outputs probabilities.

---

## Week 5 â€” Day 2 âœ…  
- ***Focus: Training & Interpreting Logistic Regression***
- Built a full preprocessing + logistic regression pipeline.
- Learned how to interpret model coefficients and what positive/negative values mean.
- Explored feature scaling impact on coefficient magnitude.
- Compared scaled vs unscaled performance.
- Visualized model decision boundary (for synthetic 2D data demo).  

âœ… **Outcome:** Ability to train logistic models and interpret how each feature influences the probability of a class.

---

## Week 5 â€” Day 3 âœ…  
- ***Focus: Model Evaluation & Metrics*** 
- Deep dive into classification metrics:
  - Accuracy
  - Precision
  - Recall
  - F1-Score
  - Specificity
- Learned to derive confusion matrix and connect each cell to metrics.
- Implemented evaluation helper functions.
- Identified the precisionâ€“recall tradeoff in imbalanced datasets.  

âœ… **Outcome:** Can properly evaluate classifier performance beyond accuracy.

---

## Week 5 â€” Day 4 âœ…  
- ***Focus: Probability Thresholds, ROC & AUC***
- Explored model.predict_proba() and how probability scores shift.
- Learned how adjusting decision threshold changes precision, recall, and F1.
- Plotted ROC curve, computed AUC, and interpreted curve shape.
- Understood TPR, FPR, and the geometry of ROC. 

âœ… **Outcome:** Can tune classification thresholds and compare models with ROC-AUC.

---

## Week 5 â€” Day 5 âœ…  
- ***Focus: Regularization â€” L1 & L2 Penalties*** 
- Learned how regularization prevents overfitting in logistic regression.
- Compared:
  - L2 (Ridge) â†’ Shrinks coefficients
  - L1 (Lasso) â†’ Can zero-out coefficients (feature selection effect)
- Trained models with different C values (regularization strength).
- Observed how coefficients shrink as regularization increases.  

âœ… **Outcome:** Ability to control model complexity and interpret regularization effects in logistic regression.

---

## Week 5 â€” Day 6 âœ…  
- ***Focus: Hyperparameter Tuning*** 
- Applied GridSearchCV and RandomizedSearchCV for logistic regression.
- Tuned parameters:
  - C
  - penalty
  - solver
- Compared performance with cross-validated mean scores.
- Extracted the best model and evaluated it on test set.

âœ… **Outcome:** Able to systematically search and select best hyperparameters for classification.

---

## Week 5 â€” Day 7 âœ…  
- ***Mini Project: Improve Titanic Survival Classifier ðŸš¢***
- Build multiple logistic models using:
  - Different scalers
  - Different regularization penalties
  - Different thresholds
- Produce a comparison table for:
  - Accuracy
  - Precision
  - Recall
  - F1
  - ROC-AUC
- Plot ROC curves of 3 models on the same plot.
- Summarize which configuration gives the best balanced performance.

âœ… **Outcome:** Completed an end-to-end classification optimization workflow using logistic regression, covering math + preprocessing + tuning + evaluation.

---

### **Core Libraries and Setup**
```bash
source .venv/bin/activate
pip install numpy pandas seaborn matplotlib scikit-learn
pip freeze > requirements.txt
```

## Key Skills

### Logistic Regression Core
- Sigmoid function
- Odds & log-odds
- Coefficients interpretation

### Evaluation
- Confusion matrix
- Precision, Recall, F1
- ROC curve, AUC
- Threshold tuning

### Optimization
- Regularization (L1, L2)
- GridSearchCV
- Cross-validation


### Examples

#### Example 1 â€” Sigmoid Function
```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

print(sigmoid(np.array([-5, 0, 5])))
```

#### Example 2 â€” Logistic Regression Pipeline
```python
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import pandas as pd

df = pd.read_csv('titanic.csv')

num_cols = ['Age', 'Fare']
cat_cols = ['Sex', 'Embarked']

tf = ColumnTransformer([
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
])

pipe = Pipeline([
    ('prep', tf),
    ('model', LogisticRegression(max_iter=1000))
])

X = df[num_cols + cat_cols]
y = df['Survived']

pipe.fit(X, y)
print("Training Accuracy:", pipe.score(X, y))

```

#### Example 3 â€” ROC Curve
```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

probs = pipe.predict_proba(X)[:, 1]
fpr, tpr, _ = roc_curve(y, probs)
auc = roc_auc_score(y, probs)

plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()
```