> [🔼 README](../../README.md)

### Week 4 — Day-by-Day Breakdown

## Week 4 — Day 1 ✅  
- **Focus:** Handling Missing Data & Outliers  
- Learned how to detect missing data using `isnull()`, `info()`, and `describe()`.  
- Practiced imputing missing values using mean, median, and mode with `SimpleImputer`.  
- Understood and detected outliers using **Z-score** and **IQR** (Interquartile Range).  
- Visualized data distributions using boxplots and histograms to identify anomalies.  

✅ **Outcome:** Able to diagnose and clean real datasets with missing or extreme values.

---

## Week 4 — Day 2 ✅  
- **Focus:** Encoding Categorical Data  
- Learned the difference between **Label Encoding** and **One-Hot Encoding**.  
- Practiced using `LabelEncoder`, `OneHotEncoder`, and `OrdinalEncoder`.  
- Combined numerical and categorical preprocessing with `ColumnTransformer`.  
- Applied encoding strategies to both binary and multi-class categorical variables.  

✅ **Outcome:** Able to convert categorical data into numerical form suitable for ML models.

---

## Week 4 — Day 3 ✅  
- **Focus:** Feature Scaling & Transformation  
- Reviewed **normalization vs standardization**.  
- Applied **StandardScaler**, **MinMaxScaler**, and **PowerTransformer**.  
- Learned how to use `Pipeline` and `ColumnTransformer` for selective scaling.  
- Observed how scaling affects different algorithms (distance-based vs tree-based).  

✅ **Outcome:** Confident in preparing heterogeneous data with consistent feature scaling.

---

## Week 4 — Day 4 ✅  
- **Focus:** Feature Selection & Importance  
- Learned various techniques for selecting meaningful features:  
  - **VarianceThreshold** — remove low-variance columns.  
  - **SelectKBest (chi2, ANOVA F-test)** — statistical feature selection.  
  - **Recursive Feature Elimination (RFE)** — iterative model-based selection.  
- Visualized how accuracy changes as features are added or removed.  

✅ **Outcome:** Can identify and keep only impactful features, improving model interpretability.

---

## Week 4 — Day 5 ✅  
- **Focus:** Intermediate Models — Random Forest & SVM  
- Introduced **RandomForestClassifier** and **Support Vector Machine (SVC)**.  
- Compared performance of tree-based vs margin-based classifiers.  
- Tuned hyperparameters like `n_estimators`, `max_depth`, `C`, and `gamma`.  
- Understood ensemble learning and overfitting control.  

✅ **Outcome:** Gained ability to train, tune, and evaluate ensemble and kernel-based models.

---

## Week 4 — Day 6 ✅  
- **Focus:** Model Evaluation Deep Dive  
- Explored **confusion matrix**, **precision-recall curves**, and **ROC-AUC**.  
- Revisited **cross-validation** for robust evaluation.  
- Understood **bias-variance trade-off** and its effect on overfitting/underfitting.  
- Learned to interpret and balance performance metrics effectively.  

✅ **Outcome:** Can critically assess and compare multiple ML models for reliability.

---

## Week 4 — Day 7 ✅  
- **Mini Project:** Titanic Survival Prediction 🛳️  
- Applied full preprocessing pipeline on the Titanic dataset:  
  - Handled missing values (`age`, `embarked`, etc.).  
  - Encoded categorical data (`sex`, `embarked`).  
  - Scaled numerical features.  
- Trained and compared multiple models:  
  - Logistic Regression  
  - RandomForestClassifier  
  - Support Vector Machine  
- Generated accuracy, F1-score, ROC-AUC, and visualized feature importance.  
- Interpreted which features most strongly influence survival prediction.

✅ **Outcome:** Completed a real-world ML project from raw data to tuned model with clear performance evaluation.

---

### **Core Libraries and Setup**
```bash
source .venv/bin/activate
pip install numpy pandas seaborn matplotlib scikit-learn
pip freeze > requirements.txt
```

## Key Skills

### Data Cleaning
- 	Handling missing values (isnull(), SimpleImputer)
- 	Outlier detection (Z-score, IQR)
- 	Boxplot & histogram analysis

### Encoding
- 	LabelEncoder, OneHotEncoder, OrdinalEncoder
- 	ColumnTransformer for mixed data

### Scaling & Transformation
- 	StandardScaler, MinMaxScaler, PowerTransformer
- 	Pipelines for consistent preprocessing

### Feature Selection
- 	VarianceThreshold, SelectKBest, RFE

### Intermediate Models
- 	RandomForestClassifier, SVC
- 	Hyperparameter tuning: GridSearchCV, RandomizedSearchCV

### Evaluation
- 	Confusion Matrix, ROC-AUC, Precision-Recall
- 	Bias-Variance trade-off

### Examples

#### Example 1 — Handling Missing Data
```python
import pandas as pd
from sklearn.impute import SimpleImputer

df = pd.DataFrame({
    'Age': [22, 38, None, 35, None],
    'Fare': [7.25, 71.83, 8.05, 53.10, 8.46]
})

imputer = SimpleImputer(strategy='mean')
df['Age'] = imputer.fit_transform(df[['Age']])
print(df)
```

#### Example 2 — Encoding Categorical Data
```python
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

df = pd.DataFrame({
    'Sex': ['male', 'female', 'female', 'male'],
    'Age': [22, 38, 26, 35],
    'Survived': [0, 1, 1, 0]
})

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), ['Sex']),
    ],
    remainder='passthrough'
)

pipe = Pipeline([
    ('pre', preprocessor),
    ('model', RandomForestClassifier())
])

X = df[['Sex', 'Age']]
y = df['Survived']
pipe.fit(X, y)
print("Training Accuracy:", pipe.score(X, y))
```

#### Example 3 — Feature Selection (SelectKBest)
```python
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest, f_classif

X, y = load_iris(return_X_y=True)
selector = SelectKBest(score_func=f_classif, k=2)
X_selected = selector.fit_transform(X, y)
print("Selected feature indices:", selector.get_support(indices=True))
```