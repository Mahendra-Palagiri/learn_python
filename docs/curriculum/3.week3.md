> [ðŸ”¼ README](../../README.md)


## Week 3 â€” Day 1 âœ…

### Focus: K-Nearest Neighbors (KNN)

- Learned the **Machine Learning Workflow** (problem definition â†’ data â†’ split â†’ model â†’ evaluation).  
- Explored the **Iris dataset** (features, targets, target names).  
- Performed **train/test split** (80/20) to avoid overfitting.  
- Implemented **KNN classifier** with scikit-learn.  
- Understood key concepts:  
  - `k` = number of neighbors to consult when predicting.  
  - Trade-off between small `k` (more flexible, but noisy) vs. large `k` (smoother, but may miss local patterns).  
  - Why stratification ensures balanced class distribution in splits.  
  - The need for **cross-validation** to select the best `k`.  
- Evaluated model using:  
  - **Accuracy**  
  - **Classification report** (precision, recall, f1-score)  
  - **Confusion matrix**  

âœ… **Outcome:** Solid grasp of KNN as a classification algorithm and how to evaluate its performance.

<!--
ðŸ“Œ Model Selection Cheat Sheet

ðŸ”¹ 1. Classification (Predict categories)
	â€¢	Logistic Regression â†’ Simple, interpretable, fast. Works well when classes are linearly separable.
	â€¢	KNN â†’ Easy to understand, good for small datasets. Sensitive to scaling.
	â€¢	Decision Tree â†’ Human-readable flowchart of decisions. Can overfit.
	â€¢	Random Forest / Gradient Boosting â†’ Stronger accuracy, handles complex data.
	â€¢	SVM â†’ Good for clear boundaries, but harder to tune.
	â€¢	Neural Networks â†’ Use only for large/complex datasets.

â¸»

ðŸ”¹ 2. Regression (Predict numbers)
	â€¢	Linear Regression â†’ Best when data follows straight-line relations.
	â€¢	Ridge / Lasso â†’ Linear regression with penalties to avoid overfitting.
	â€¢	Decision Tree Regressor â†’ Can capture non-linear patterns.
	â€¢	Random Forest / Gradient Boosting â†’ Stronger for complex data, less interpretable.

â¸»

ðŸ”¹ 3. Unsupervised Learning (No labels)
	â€¢	KMeans â†’ Simple, fast clustering. Needs you to pick number of clusters.
	â€¢	DBSCAN / Hierarchical â†’ Detects clusters of varying shapes and sizes.
	â€¢	PCA â†’ Reduce dataset dimensions, keep main information.
	â€¢	t-SNE / UMAP â†’ Great for visualization of high-dimensional data.

-->
