> [ðŸ”¼ README](../../README.md)

### Week 3 â€” Day-by-Day Breakdown

## Week 3 â€” Day 1 âœ…
- Focus: Introduction to Machine Learning.  
- Understood the core idea of machine learning and model training workflow.  
- Learned about supervised vs unsupervised learning, classification vs regression, and the concept of features and labels.
- Worked with the classic Iris dataset using scikit-learn.
- Visualized dataset relationships using seaborn.pairplot().


âœ… Outcome: Built strong conceptual understanding of ML fundamentals and dataset exploration.


## Week 3 â€” Day 2 âœ…
- 	Focus: Training Classification Models (KNN, Logistic Regression, Decision Tree)
- 	Learned how to split data into training and test sets using train_test_split.
- 	Implemented K-Nearest Neighbors (KNN) and explored the effect of k.
- 	Trained and evaluated Logistic Regression and Decision Tree models.
- 	Compared model performance using accuracy score and classification report.

âœ… Outcome: Gained hands-on understanding of model training, evaluation, and when to choose which classifier.


## Week 3 â€” Day 3 âœ…
- 	Focus: Scaling and Model Comparison
- 	Understood the concept of feature scaling and why itâ€™s essential for distance-based algorithms.
- 	Practiced using StandardScaler and integrated scaling via Pipelines.
- 	Compared models (KNN, Logistic Regression, Decision Tree) before and after scaling.
- 	Learned to interpret classification_report (precision, recall, f1-score).

âœ… Outcome: Learned how preprocessing affects model accuracy and built reproducible workflows with pipelines.



## Week 3 â€” Day 4 âœ…
- 	Focus: Model Evaluation and Feature Importance
- 	Studied ROC curves, AUC, and confusion matrices for deeper evaluation.
- 	Learned about feature importance â€” using coef_ (for Logistic Regression) and feature_importances_ (for Decision Tree).
- 	Visualized feature importance to identify key predictive variables.
- 	Compared models based on interpretability and use-case suitability.

âœ… Outcome: Ability to interpret model quality beyond raw accuracy and identify important features.


## Week 3 â€” Day 5 âœ…
- 	Focus: Model Optimization
- 	Introduced cross-validation, GridSearchCV, and RandomizedSearchCV for hyperparameter tuning.
- 	Explored k-fold validation and how it improves model reliability.
- 	Learned how to extract best_params_ and best_estimator_ for optimized models.
- 	Understood trade-offs between exhaustive search and randomized search.

âœ… Outcome: Learned to improve and validate ML models systematically.


## Week 3 â€” Day 6â€“7
- 	Focus: Weekly Challenge â€” â€œIris Classifier Improvement Labâ€
- 	Combine all concepts (scaling, model comparison, tuning, evaluation).
- 	Build a pipeline that automatically selects the best classifier.
- 	Produce classification reports and ROC curves for all tuned models.
- 	Document findings and conclusions.

âœ… Outcome: Able to take a real dataset through every stage â€” from preprocessing to model tuning and evaluation.


Core Libraries and Setup
```bash
source .venv/bin/activate
pip install scikit-learn matplotlib seaborn numpy pandas
pip freeze > requirements.txt
```

## Key Skills

### Machine Learning Concepts
- 	Supervised vs Unsupervised Learning
- 	Classification vs Regression
- 	Features, Labels, Train/Test Split
- 	Evaluation Metrics: Accuracy, Precision, Recall, F1-score
- 	ROC, AUC, Confusion Matrix
- 	Feature Importance and Interpretability
- 	Cross-validation, GridSearchCV, RandomizedSearchCV

### scikit-learn
- 	Datasets (load_iris)
- 	Models: KNN, Logistic Regression, Decision Tree
- 	Preprocessing: StandardScaler, Pipeline
- 	Model Tuning: GridSearchCV, RandomizedSearchCV
- 	Metrics: classification_report, roc_curve, roc_auc_score

### Examples

#### KNN Example
```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
preds = knn.predict(X_test)
print("Accuracy:", accuracy_score(y_test, preds))
```

#### Feature Scaling with Pipeline
```python
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression())
])
pipeline.fit(X_train, y_train)
print("Test Accuracy:", pipeline.score(X_test, y_test))
```

#### GridSearchCV Example
```python
from sklearn.model_selection import GridSearchCV

param_grid = {'n_neighbors': [3, 5, 7, 9]}
grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
grid.fit(X_train, y_train)
print("Best Params:", grid.best_params_)
print("Best Score:", grid.best_score_)
```