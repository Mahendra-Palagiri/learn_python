'''

------------------------------------------------------------------------------------------------------------
üåÖ Week 4 ‚Äì Day 2: Data Transformation & Feature Scaling

üéØ Learning Goals

By the end of today, you‚Äôll be able to:
	1.	Explain why raw numeric features (e.g. Age, Fare, Income, etc.) can distort models if not scaled.
	2.	Apply standardization and normalization using sklearn.preprocessing.
	3.	Understand how scaling affects distance-based models (like KNN or clustering).
	4.	Use Pipeline for safe and reproducible scaling (so you don‚Äôt leak data).
	5.	Visualize before/after scaling distributions.
------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------
üß© Concept Overview

Concept                             What It Means                                           Analogy
-------                             --------------                                          ----------
Standardization                     Transform features so they                              Like shifting and stretching data 
                                    have mean = 0 and std = 1                               onto a ‚Äúcommon measuring stick.‚Äù 
     
Normalization (Min-Max Scaling)     Scale all values into range [0, 1]                      Like resizing all photos to the same 
                                                                                            frame size before comparison.

RobustScaler                        Uses median & IQR instead of mean/std                   More resistant to outliers.

When to Use                         ‚Ä¢ KNN, SVM, Logistic Regression, PCA need scaling
                                    ‚Ä¢ Tree-based models (DecisionTree, RandomForest) don‚Äôt.
------------------------------------------------------------------------------------------------------------


'''


import pandas as pd
from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Import the dataset
titdf = pd.read_csv("./data/week4/titanic.csv")
print("\n\n------ DATA FRAME INFO --------\n\n")
print(titdf.info(),"\n\n")

# 2. Clean numeric columns
num_cols = ['Fare','Age']
filt_num_data = titdf[num_cols].dropna() # drop NaN for clean comparison

print("\n\n -- Min Fare --- > ",filt_num_data['Fare'].min())
print("\n\n -- Max Fare --- > ",filt_num_data['Fare'].max())
print("\n\n -- mean Fare --- > ",filt_num_data['Fare'].mean())
print("\n\n -- median Fare --- > ",filt_num_data['Fare'].median())
print("\n\n -- std deviation in Fare --- > ",filt_num_data['Fare'].std())
print("\n\n -- Min Age --- > ",filt_num_data['Age'].min())
print("\n\n -- Max Age --- > ",filt_num_data['Age'].max())
print("\n\n -- mean Age --- > ",filt_num_data['Age'].mean())
print("\n\n -- median Age --- > ",filt_num_data['Age'].median())
print("\n\n -- std deviation in Age --- > ",filt_num_data['Age'].std())

# 3. Initialize and set different scalers
std_scaler = StandardScaler()
minmax_scaler = MinMaxScaler()
robust_scaler = RobustScaler()

X_std = std_scaler.fit_transform(filt_num_data)
X_minmax = minmax_scaler.fit_transform(filt_num_data)
X_robust = robust_scaler.fit_transform(filt_num_data)

# 4. Visualize data
fig, axes = plt.subplots(1, 4, figsize=(15,5))
sns.kdeplot(filt_num_data['Fare'], ax=axes[0], label='Original')
sns.kdeplot(X_std[:,1], ax=axes[1], label='Standardized')
sns.kdeplot(X_minmax[:,1], ax=axes[2], label='Normalized')
sns.kdeplot(X_robust[:,1], ax=axes[3], label='Robust')
for ax in axes: ax.legend()
plt.show()

# 5. Optimized and Final Setup
y = titdf.loc[filt_num_data.index, 'Survived']
X = filt_num_data  # only ['Fare','Age'] with no NaNs

preprocess = ColumnTransformer(
    transformers=[
    ('age_scaler',StandardScaler(),['Age']),
    ('fare_scaler',RobustScaler(),['Fare'])
	],
    remainder='drop'
)

final_pipe = Pipeline([
    ('prep', preprocess),
    ('clf',  LogisticRegression(max_iter=1000, random_state=42))
])

final_pipe.fit(X, y)

# 1Ô∏è‚É£ Get only the preprocessing part from the pipeline
scaler = final_pipe.named_steps['prep']

# 2Ô∏è‚É£ Transform your original X data (Age & Fare)
X_scaled = scaler.transform(X)

# 3Ô∏è‚É£ Find out what columns came out of the scaler
col_names = scaler.get_feature_names_out()
print(col_names)
# Example output: ['age_scaler__Age', 'fare_scaler__Fare']

# 4Ô∏è‚É£ Turn the scaled NumPy array into a small DataFrame
X_scaled_df = pd.DataFrame(X_scaled, columns=col_names, index=X.index)

# 5Ô∏è‚É£ Plot using Seaborn
sns.kdeplot(x=X_scaled_df[col_names[0]], label='Age (scaled)')
sns.kdeplot(x=X_scaled_df[col_names[1]], label='Fare (scaled)')
plt.legend()
plt.title("KDE Plot of Scaled Features")
plt.show()

'''
--------------------- RETROSPECTION --------------------

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
üß© 1Ô∏è‚É£ Why drop NaN values from numeric columns (instead of imputing)?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When we wrote:
X = df[['Age', 'Fare']].dropna()

we are not saying ‚Äúignore missing values forever.‚Äù
We‚Äôre saying: ‚Äúfor the purpose of demonstrating scaling math (z-score, min-max, etc.), let‚Äôs temporarily work with complete data.‚Äù
Because:


Reason                                    	Explanation
------------								--------------
üßÆ Scaling formulas need real numbers		The formula for standardization (x - Œº)/œÉ cannot handle NaN ‚Äî even one missing value breaks the computation.

üß™ Today‚Äôs goal is to see how scaling       We already learned imputation yesterday (Day 1). 
changes numeric distributions, 				Today‚Äôs focus is transformation of numeric values, so we use ‚Äúclean numeric columns‚Äù to avoid mixing concepts.
not how to clean data											

‚öôÔ∏è Scaling should happen after 				In a real ML workflow, you first impute ‚Üí then scale. 
imputation in a real pipeline				But for a conceptual demo, we skip the imputation part to directly visualize scaling math.

So dropna() here is a convenience for visualization, not a data-cleaning recommendation.


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
üß© 2Ô∏è‚É£ Why only Age and Fare and not Survived?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Because these are the continuous numeric features in the Titanic dataset.
Here‚Äôs how each variable type breaks down:

Column					Type						Nature							Scaling Needed?		 Why / Why Not
--------				-------						----------						---------			 ----------------
Age						Numeric (continuous)		People‚Äôs ages (0‚Äì80s)			‚úÖ Yes				Different range than other numeric features
Fare					Numeric (continuous)		Ticket prices (0‚Äì500+)			‚úÖ Yes				Highly skewed, large variance
Survived				Binary categorical (0/1)	Target label					‚ùå No				It‚Äôs the outcome, not an input feature
Embarked, Sex, Cabin	Categorical (non-numeric)	Port, gender, cabin string		‚ùå No				Need encoding (like OneHot) before numeric scaling

üëâ So only Age and Fare are ‚Äútrue‚Äù numeric features suitable for scaling.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
üß© 3Ô∏è‚É£ Why not impute missing numeric values instead of dropping?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Yes, in production or modeling, you always impute before scaling.

So, the correct ‚Äúreal-world‚Äù order would be:

1Ô∏è‚É£ Detect missing values
2Ô∏è‚É£ Impute them
3Ô∏è‚É£ Then apply scaling

For example:
```python
	from sklearn.impute import SimpleImputer
	from sklearn.preprocessing import StandardScaler
	from sklearn.pipeline import Pipeline

	num_pipe = Pipeline([
		('impute', SimpleImputer(strategy='mean')),
		('scale', StandardScaler())
	])

	X_scaled = num_pipe.fit_transform(df[['Age', 'Fare']])
```
For our learning , we‚Äôre intentionally doing scaling on clean data first to:
	‚Ä¢	Understand how scaling behaves mathematically,
	‚Ä¢	Avoid introducing extra noise from imputation (which can change mean/std),
	‚Ä¢	Keep visualizations simple when we plot before/after scaling.
    

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
üß© Explain how StandardScaler, MinMaxScaler and RobustScaler differ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

üß© 1Ô∏è‚É£ Why scaling matters in the first place

	Let‚Äôs say your dataset has:
	Feature		Typical Range	Example Values
	-------		------------	---------------
	Age			0 ‚Äì 80			20, 40, 60
	Fare		0 ‚Äì 500			50, 250, 400

	If you feed this directly into an ML model that uses distance (like KNN, SVM, Logistic Regression),
	the model will think Fare is ‚Äúmore important‚Äù than Age ‚Äî simply because 500 > 80.

	So, scaling brings everything to a common scale before the algorithm computes distances or gradients.

		üß© When does a model look across columns?

			Only certain models do ‚Äî specifically, models that measure ‚Äúdistance‚Äù or magnitude differences across all features together.

			Let‚Äôs take an example to visualize this.

			Suppose you have these two data points (rows in a dataset):
            Passenger	Age	Fare
            --------	--	----‚àö
			A			20	40
			B			40	400
            
            Without scaling:

			The Euclidean distance between A and B is:
						 --------------------------------	
			distance =  ‚àö  (20-40)¬≤ +  (40-400)¬≤
            so distance is approx ~ --> 360
            
			Even though Age differs by 20 years, that small 20-point difference is completely dwarfed by the Fare difference (360 units).

			So, when your model looks at the overall distance:
				‚Ä¢	99% of that ‚Äúdistance‚Äù comes from Fare,
				‚Ä¢	Age is almost ignored.
        
        Summary : Ensure that all the columns are scaled properly so that any models that measure distance doesnt provide wrong results.

üß© 2Ô∏è‚É£ Checking how the scalers work with and wihtout outliers 

‚öôÔ∏è Step 1: The dataset without outliers

	Let‚Äôs start simple:

	Passenger	Age			Fare
	---------	---			-----
	A			20			40
	B			30			60
	C			40			80
	D			50			100

	‚úÖ Everything looks ‚Äúnormal-ish.‚Äù
	We can compute scaling comfortably.

    *** üìò StandardScaler
    
	Z = (x - Œº)/ùõî
     
    Mean (Œº) and Std (œÉ):
    Feature		Œº		œÉ
    ------		--		---
	Age			35		12.91
	Fare		70		25.82
    
    Calculate the Z-Score
    Passenger	Age		Age_scaled		Fare	Fare_scaled
    -------		---		---------		---		------------
	A			20		‚àí1.16			40		‚àí1.16
	B			30		‚àí0.39			60		‚àí0.39
	C			40		+0.39			80		+0.39
	D			50		+1.16			100		+1.16

    Perfectly centered. ‚úÖ

	*** üìò MinMaxScaler

	Formula:
	x` = (x - min) / (max - min)
    
    Passenger	Age		Age_scaled	Fare	Fare_scaled
    --------	---		---------	----	-----------
	A			20		0.0			40		0.0
	B			30		0.33		60		0.33
	C			40		0.67		80		0.67
	D			50		1.0			100		1.0

    All values between 0 and 1, perfect spread. ‚úÖ


	*** üìò RobustScaler

	Uses median (Q2) and IQR = Q3 -	Q1.
    Feature		Median (Q2)		Q1		Q3		IQR (Q3‚àíQ1)
    -------		----------		---		--		-----------
	Age			35				27.5	45		17.5
	Fare		70				55		85		30

	Formula:
	x` = (x - Q2)/IQR
	Passenger	Age		Age_scaled	Fare	Fare_scaled
    ---------	---		---------	----	------------
	A			20		‚àí0.86		40		‚àí1.00
	B			30		‚àí0.29		60		‚àí0.33
	C			40		+0.29		80		+0.33
	D			50		+0.86		100		+1.00

	Everything‚Äôs stable and clean. ‚úÖ
    
‚ö†Ô∏è Step 2: Now, introduce an outlier

	Let‚Äôs add Passenger E with a huge Fare = 1000.
    
	Passenger	Age		Fare		
    --------	---		-----
    A			20		40
	B			30		60
	C			40		80
    D			50		100
	E			50		1000 ‚üµ OUTLIER!

    Now let‚Äôs see what happens.

	üö® StandardScaler with outlier

	New mean and std for Fare:
		‚Ä¢	Mean = (40 + 60 + 80 + 100 + 1000)/5 = 256
		‚Ä¢	Std ‚âà 385

	Apply scaling:
    Passenger	Fare	Scaled
    --------	----	-------
	A			40		(40‚àí256)/385 = ‚àí0.56
	B			60		‚àí0.51
	C			80		‚àí0.46	
    D			100		‚àí0.40
	E			1000	+1.93
    
    üò¨ What happened?
	‚Ä¢	Because of one huge value (1000), the mean and std exploded,
	‚Ä¢	making the other 4 ‚Äúnormal‚Äù values appear very close to 0, even though they were spread between 40‚Äì100.
	‚ö†Ô∏è StandardScaler gets ‚Äúdistracted‚Äù by outliers.
    
    

	üö® MinMaxScaler with outlier

	Min = 40, Max = 1000
	Formula ‚Üí (x‚àí40)/(1000‚àí40)
    
    Passenger	Fare	Scaled
    --------	---		------
	A			40		0.0
	B			60		0.02
	C			80		0.04	
    D			100		0.06
	E			1000	1.0

    üò¨ Now everything except the outlier is squished into 0‚Äì0.06 range.
	The model thinks all normal fares are ‚Äúbasically the same.‚Äù
	‚ö†Ô∏è MinMaxScaler also suffers badly from outliers.
    
    
    
    ‚úÖ RobustScaler with outlier

	Median (Q2) = 80
	Q1 = 60, Q3 = 100, IQR = 40

	Formula ‚Üí (x‚àí80)/40
    
	Passenger	Fare	Scaled
    --------	----	------	
	A			40		‚àí1.00
	B			60		‚àí0.50
	C			80		0.00
	D			100		+0.50
	E			1000	+23.00

	‚ú® Look what happens:
	‚Ä¢	The ‚Äúnormal‚Äù range (40‚Äì100) still stays between ‚àí1 and +1.
	‚Ä¢	The outlier (1000) goes way off (+23) but doesn‚Äôt distort everyone else‚Äôs scale.
	RobustScaler keeps the central 50% stable.
    
    Summary for Fare :
    Without outlier:
		StandardScaler:   -1.2  -0.4  0.4  1.2
		MinMaxScaler:     0.0   0.3   0.6  1.0
		RobustScaler:    -0.8  -0.3   0.3  0.8

	With outlier (Fare=1000):
		StandardScaler:   -0.56  -0.51  -0.46  -0.40  +1.93
		MinMaxScaler:     0.00   0.02   0.04   0.06   1.00
		RobustScaler:    -1.00  -0.50   0.00   0.50  +23.00
        
    ‚Ä¢	StandardScaler and MinMaxScaler ‚Äúshrink‚Äù everything because the outlier distorts their math.
	‚Ä¢	RobustScaler keeps normal values meaningful ‚Äî it only lets the outlier stretch by itself.
    
    üß© Step 4: Summary Table
    
    Scaler				Uses			Center		Spread			Outlier Effect		 Good For
    -------				-----			-----		------			--------------		 ----------
	StandardScaler		mean, std		mean=0		std=1			‚ùå Large impact		Bell-shaped data
	MinMaxScaler		min, max		0‚Äì1 range	range-based		‚ùå Very sensitive	Bounded features
	RobustScaler		median, IQR		median=0	IQR=1			‚úÖ Minimal			Outlier-heavy or skewed data

    ************* A quick rule of thumb to select which scaler to be applied 
    
    Relationship between Mean & Median	Interpretation			Implication
    ----------------------------------	--------------			-------------
	Mean ‚âà Median						Symmetric (normal-ish)	‚úÖ StandardScaler fine
	Mean >> Median						Right-skewed			‚ö†Ô∏è Use RobustScaler or log-transform
	Mean << Median						Left-skewed (rare)		‚ö†Ô∏è Use RobustScaler
    
    In our Titanic dataset
    * Min Fare --- >  13.568895885370686
	* Max Fare --- >  250.0
	* Mean Fare --- >  47.20567742127721
	* Median Fare --- >  42.0616928774529
	* Std deviation in Fare --- >  33.53599558985873
    
	* Min Age --- >  0.4
	* Max Age --- >  53.19996122179421
	* Mean Age --- >  26.7872039001908
	* Median Age --- >  26.671436298735237
	* Std deviation in Age --- >  14.18943084053789.  
    
    üß† Fare:
	‚Ä¢	Mean (47.2) > Median (42.0)
	‚Ä¢	That‚Äôs about a 12% gap
	‚Ä¢	Also note: Max = 250, roughly 5√ó the mean

		‚úÖ Interpretation:
			‚Ä¢	Right-skewed ‚Äî a few passengers paid extremely high fares
			‚Ä¢	Likely some first-class / luxury ticket outliers

		üëâ Use RobustScaler for Fare.
	

	üß† Age:
		‚Ä¢	Mean (26.8) ‚âà Median (26.7)
		‚Ä¢	That‚Äôs less than 0.5% difference
		‚Ä¢	Range is small and reasonable: 0.4 to 53 (no massive outlier)

		‚úÖ Interpretation:
			‚Ä¢	Fairly symmetric
			‚Ä¢	No heavy tails

		üëâ Use StandardScaler for Age.

    
'''
