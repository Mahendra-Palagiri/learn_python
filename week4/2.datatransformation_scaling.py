'''

------------------------------------------------------------------------------------------------------------
ğŸŒ… Week 4 â€“ Day 2: Data Transformation & Feature Scaling

ğŸ¯ Learning Goals

By the end of today, youâ€™ll be able to:
	1.	Explain why raw numeric features (e.g. Age, Fare, Income, etc.) can distort models if not scaled.
	2.	Apply standardization and normalization using sklearn.preprocessing.
	3.	Understand how scaling affects distance-based models (like KNN or clustering).
	4.	Use Pipeline for safe and reproducible scaling (so you donâ€™t leak data).
	5.	Visualize before/after scaling distributions.
------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------
ğŸ§© Concept Overview

Concept                             What It Means                                           Analogy
-------                             --------------                                          ----------
Standardization                     Transform features so they                              Like shifting and stretching data 
                                    have mean = 0 and std = 1                               onto a â€œcommon measuring stick.â€ 
     
Normalization (Min-Max Scaling)     Scale all values into range [0, 1]                      Like resizing all photos to the same 
                                                                                            frame size before comparison.

RobustScaler                        Uses median & IQR instead of mean/std                   More resistant to outliers.

When to Use                         â€¢ KNN, SVM, Logistic Regression, PCA need scaling
                                    â€¢ Tree-based models (DecisionTree, RandomForest) donâ€™t.
------------------------------------------------------------------------------------------------------------


'''


import pandas as pd
from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Import the dataset
titdf = pd.read_csv("./data/week4/titanic.csv")
print("\n\n------ DATA FRAME INFO --------\n\n")
print(titdf.info(),"\n\n")

# 2. Clean numeric columns
num_cols = ['Fare','Age']
filt_num_data = titdf[num_cols].dropna() # drop NaN for clean comparison

print("\n\n -- Min Fare --- > ",filt_num_data['Fare'].min())
print("\n\n -- Max Fare --- > ",filt_num_data['Fare'].max())
print("\n\n -- mean Fare --- > ",filt_num_data['Fare'].mean())
print("\n\n -- median Fare --- > ",filt_num_data['Fare'].median())
print("\n\n -- std deviation in Fare --- > ",filt_num_data['Fare'].std())
print("\n\n -- Min Age --- > ",filt_num_data['Age'].min())
print("\n\n -- Max Age --- > ",filt_num_data['Age'].max())
print("\n\n -- mean Age --- > ",filt_num_data['Age'].mean())
print("\n\n -- median Age --- > ",filt_num_data['Age'].median())
print("\n\n -- std deviation in Age --- > ",filt_num_data['Age'].std())

# 3. Initialize and set different scalers
std_scaler = StandardScaler()
minmax_scaler = MinMaxScaler()
robust_scaler = RobustScaler()

X_std = std_scaler.fit_transform(filt_num_data)
X_minmax = minmax_scaler.fit_transform(filt_num_data)
X_robust = robust_scaler.fit_transform(filt_num_data)

# 4. Visualize data
fig, axes = plt.subplots(1, 4, figsize=(15,5))
sns.kdeplot(filt_num_data['Fare'], ax=axes[0], label='Original')
sns.kdeplot(X_std[:,1], ax=axes[1], label='Standardized')
sns.kdeplot(X_minmax[:,1], ax=axes[2], label='Normalized')
sns.kdeplot(X_robust[:,1], ax=axes[3], label='Robust')
for ax in axes: ax.legend()
plt.show()

# 5. Optimized and Final Setup
y = titdf.loc[filt_num_data.index, 'Survived']
X = filt_num_data  # only ['Fare','Age'] with no NaNs

preprocess = ColumnTransformer(
    transformers=[
    ('age_scaler',StandardScaler(),['Age']),
    ('fare_scaler',RobustScaler(),['Fare'])
	],
    remainder='drop'
)

final_pipe = Pipeline([
    ('prep', preprocess),
    ('clf',  LogisticRegression(max_iter=1000, random_state=42))
])

final_pipe.fit(X, y)

# 1ï¸âƒ£ Get only the preprocessing part from the pipeline
scaler = final_pipe.named_steps['prep']

# 2ï¸âƒ£ Transform your original X data (Age & Fare)
X_scaled = scaler.transform(X)

# 3ï¸âƒ£ Find out what columns came out of the scaler
col_names = scaler.get_feature_names_out()
print(col_names)
# Example output: ['age_scaler__Age', 'fare_scaler__Fare']

# 4ï¸âƒ£ Turn the scaled NumPy array into a small DataFrame
X_scaled_df = pd.DataFrame(X_scaled, columns=col_names, index=X.index)

# 5ï¸âƒ£ Plot using Seaborn
sns.kdeplot(x=X_scaled_df[col_names[0]], label='Age (scaled)')
sns.kdeplot(x=X_scaled_df[col_names[1]], label='Fare (scaled)')
plt.legend()
plt.title("KDE Plot of Scaled Features")
plt.show()

'''
--------------------- RETROSPECTION --------------------

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ğŸ§© 1ï¸âƒ£ Why drop NaN values from numeric columns (instead of imputing)?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When we wrote:
X = df[['Age', 'Fare']].dropna()

we are not saying â€œignore missing values forever.â€
Weâ€™re saying: â€œfor the purpose of demonstrating scaling math (z-score, min-max, etc.), letâ€™s temporarily work with complete data.â€
Because:


Reason                                    	Explanation
------------								--------------
ğŸ§® Scaling formulas need real numbers		The formula for standardization (x - Î¼)/Ïƒ cannot handle NaN â€” even one missing value breaks the computation.

ğŸ§ª Todayâ€™s goal is to see how scaling       We already learned imputation yesterday (Day 1). 
changes numeric distributions, 				Todayâ€™s focus is transformation of numeric values, so we use â€œclean numeric columnsâ€ to avoid mixing concepts.
not how to clean data											

âš™ï¸ Scaling should happen after 				In a real ML workflow, you first impute â†’ then scale. 
imputation in a real pipeline				But for a conceptual demo, we skip the imputation part to directly visualize scaling math.

So dropna() here is a convenience for visualization, not a data-cleaning recommendation.


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ğŸ§© 2ï¸âƒ£ Why only Age and Fare and not Survived?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Because these are the continuous numeric features in the Titanic dataset.
Hereâ€™s how each variable type breaks down:

Column					Type						Nature							Scaling Needed?		 Why / Why Not
--------				-------						----------						---------			 ----------------
Age						Numeric (continuous)		Peopleâ€™s ages (0â€“80s)			âœ… Yes				Different range than other numeric features
Fare					Numeric (continuous)		Ticket prices (0â€“500+)			âœ… Yes				Highly skewed, large variance
Survived				Binary categorical (0/1)	Target label					âŒ No				Itâ€™s the outcome, not an input feature
Embarked, Sex, Cabin	Categorical (non-numeric)	Port, gender, cabin string		âŒ No				Need encoding (like OneHot) before numeric scaling

ğŸ‘‰ So only Age and Fare are â€œtrueâ€ numeric features suitable for scaling.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ğŸ§© 3ï¸âƒ£ Why not impute missing numeric values instead of dropping?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Yes, in production or modeling, you always impute before scaling.

So, the correct â€œreal-worldâ€ order would be:

1ï¸âƒ£ Detect missing values
2ï¸âƒ£ Impute them
3ï¸âƒ£ Then apply scaling

For example:
```python
	from sklearn.impute import SimpleImputer
	from sklearn.preprocessing import StandardScaler
	from sklearn.pipeline import Pipeline

	num_pipe = Pipeline([
		('impute', SimpleImputer(strategy='mean')),
		('scale', StandardScaler())
	])

	X_scaled = num_pipe.fit_transform(df[['Age', 'Fare']])
```
For our learning , weâ€™re intentionally doing scaling on clean data first to:
	â€¢	Understand how scaling behaves mathematically,
	â€¢	Avoid introducing extra noise from imputation (which can change mean/std),
	â€¢	Keep visualizations simple when we plot before/after scaling.
    

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ğŸ§© Explain how StandardScaler, MinMaxScaler and RobustScaler differ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ğŸ§© 1ï¸âƒ£ Why scaling matters in the first place

	Letâ€™s say your dataset has:
	Feature		Typical Range	Example Values
	-------		------------	---------------
	Age			0 â€“ 80			20, 40, 60
	Fare		0 â€“ 500			50, 250, 400

	If you feed this directly into an ML model that uses distance (like KNN, SVM, Logistic Regression),
	the model will think Fare is â€œmore importantâ€ than Age â€” simply because 500 > 80.

	So, scaling brings everything to a common scale before the algorithm computes distances or gradients.

		ğŸ§© When does a model look across columns?

			Only certain models do â€” specifically, models that measure â€œdistanceâ€ or magnitude differences across all features together.

			Letâ€™s take an example to visualize this.

			Suppose you have these two data points (rows in a dataset):
            Passenger	Age	Fare
            --------	--	----âˆš
			A			20	40
			B			40	400
            
            Without scaling:

			The Euclidean distance between A and B is:
						 --------------------------------	
			distance =  âˆš  (20-40)Â² +  (40-400)Â²
            so distance is approx ~ --> 360
            
			Even though Age differs by 20 years, that small 20-point difference is completely dwarfed by the Fare difference (360 units).

			So, when your model looks at the overall distance:
				â€¢	99% of that â€œdistanceâ€ comes from Fare,
				â€¢	Age is almost ignored.
        
        Summary : Ensure that all the columns are scaled properly so that any models that measure distance doesnt provide wrong results.

ğŸ§© 2ï¸âƒ£ Checking how the scalers work with and wihtout outliers 

âš™ï¸ Step 1: The dataset without outliers

	Letâ€™s start simple:

	Passenger	Age			Fare
	---------	---			-----
	A			20			40
	B			30			60
	C			40			80
	D			50			100

	âœ… Everything looks â€œnormal-ish.â€
	We can compute scaling comfortably.

    *** ğŸ“˜ StandardScaler
    
	Z = (x - Î¼)/ğ›”
     
    Mean (Î¼) and Std (Ïƒ):
    Feature		Î¼		Ïƒ
    ------		--		---
	Age			35		12.91
	Fare		70		25.82
    
    Calculate the Z-Score
    Passenger	Age		Age_scaled		Fare	Fare_scaled
    -------		---		---------		---		------------
	A			20		âˆ’1.16			40		âˆ’1.16
	B			30		âˆ’0.39			60		âˆ’0.39
	C			40		+0.39			80		+0.39
	D			50		+1.16			100		+1.16

    Perfectly centered. âœ…

	*** ğŸ“˜ MinMaxScaler

	Formula:
	x` = (x - min) / (max - min)
    
    Passenger	Age		Age_scaled	Fare	Fare_scaled
    --------	---		---------	----	-----------
	A			20		0.0			40		0.0
	B			30		0.33		60		0.33
	C			40		0.67		80		0.67
	D			50		1.0			100		1.0

    All values between 0 and 1, perfect spread. âœ…


	*** ğŸ“˜ RobustScaler

	Uses median (Q2) and IQR = Q3 -	Q1.
    Feature		Median (Q2)		Q1		Q3		IQR (Q3âˆ’Q1)
    -------		----------		---		--		-----------
	Age			35				27.5	45		17.5
	Fare		70				55		85		30

	Formula:
	x` = (x - Q2)/IQR
	Passenger	Age		Age_scaled	Fare	Fare_scaled
    ---------	---		---------	----	------------
	A			20		âˆ’0.86		40		âˆ’1.00
	B			30		âˆ’0.29		60		âˆ’0.33
	C			40		+0.29		80		+0.33
	D			50		+0.86		100		+1.00

	Everythingâ€™s stable and clean. âœ…
    
âš ï¸ Step 2: Now, introduce an outlier

	Letâ€™s add Passenger E with a huge Fare = 1000.
    
	Passenger	Age		Fare		
    --------	---		-----
    A			20		40
	B			30		60
	C			40		80
    D			50		100
	E			50		1000 âŸµ OUTLIER!

    Now letâ€™s see what happens.

	ğŸš¨ StandardScaler with outlier

	New mean and std for Fare:
		â€¢	Mean = (40 + 60 + 80 + 100 + 1000)/5 = 256
		â€¢	Std â‰ˆ 385

	Apply scaling:
    Passenger	Fare	Scaled
    --------	----	-------
	A			40		(40âˆ’256)/385 = âˆ’0.56
	B			60		âˆ’0.51
	C			80		âˆ’0.46	
    D			100		âˆ’0.40
	E			1000	+1.93
    
    ğŸ˜¬ What happened?
	â€¢	Because of one huge value (1000), the mean and std exploded,
	â€¢	making the other 4 â€œnormalâ€ values appear very close to 0, even though they were spread between 40â€“100.
	âš ï¸ StandardScaler gets â€œdistractedâ€ by outliers.
    
    

	ğŸš¨ MinMaxScaler with outlier

	Min = 40, Max = 1000
	Formula â†’ (xâˆ’40)/(1000âˆ’40)
    
    Passenger	Fare	Scaled
    --------	---		------
	A			40		0.0
	B			60		0.02
	C			80		0.04	
    D			100		0.06
	E			1000	1.0

    ğŸ˜¬ Now everything except the outlier is squished into 0â€“0.06 range.
	The model thinks all normal fares are â€œbasically the same.â€
	âš ï¸ MinMaxScaler also suffers badly from outliers.
    
    
    
    âœ… RobustScaler with outlier

	Median (Q2) = 80
	Q1 = 60, Q3 = 100, IQR = 40

	Formula â†’ (xâˆ’80)/40
    
	Passenger	Fare	Scaled
    --------	----	------	
	A			40		âˆ’1.00
	B			60		âˆ’0.50
	C			80		0.00
	D			100		+0.50
	E			1000	+23.00

	âœ¨ Look what happens:
	â€¢	The â€œnormalâ€ range (40â€“100) still stays between âˆ’1 and +1.
	â€¢	The outlier (1000) goes way off (+23) but doesnâ€™t distort everyone elseâ€™s scale.
	RobustScaler keeps the central 50% stable.
    
    Summary for Fare :
    Without outlier:
		StandardScaler:   -1.2  -0.4  0.4  1.2
		MinMaxScaler:     0.0   0.3   0.6  1.0
		RobustScaler:    -0.8  -0.3   0.3  0.8

	With outlier (Fare=1000):
		StandardScaler:   -0.56  -0.51  -0.46  -0.40  +1.93
		MinMaxScaler:     0.00   0.02   0.04   0.06   1.00
		RobustScaler:    -1.00  -0.50   0.00   0.50  +23.00
        
    â€¢	StandardScaler and MinMaxScaler â€œshrinkâ€ everything because the outlier distorts their math.
	â€¢	RobustScaler keeps normal values meaningful â€” it only lets the outlier stretch by itself.
    
    ğŸ§© Step 4: Summary Table
    
    Scaler				Uses			Center		Spread			Outlier Effect		 Good For
    -------				-----			-----		------			--------------		 ----------
	StandardScaler		mean, std		mean=0		std=1			âŒ Large impact		Bell-shaped data
	MinMaxScaler		min, max		0â€“1 range	range-based		âŒ Very sensitive	Bounded features
	RobustScaler		median, IQR		median=0	IQR=1			âœ… Minimal			Outlier-heavy or skewed data

    ************* A quick rule of thumb to select which scaler to be applied 
    
    Relationship between Mean & Median	Interpretation			Implication
    ----------------------------------	--------------			-------------
	Mean â‰ˆ Median						Symmetric (normal-ish)	âœ… StandardScaler fine
	Mean >> Median						Right-skewed			âš ï¸ Use RobustScaler or log-transform
	Mean << Median						Left-skewed (rare)		âš ï¸ Use RobustScaler
    
    In our Titanic dataset
    * Min Fare --- >  13.568895885370686
	* Max Fare --- >  250.0
	* Mean Fare --- >  47.20567742127721
	* Median Fare --- >  42.0616928774529
	* Std deviation in Fare --- >  33.53599558985873
    
	* Min Age --- >  0.4
	* Max Age --- >  53.19996122179421
	* Mean Age --- >  26.7872039001908
	* Median Age --- >  26.671436298735237
	* Std deviation in Age --- >  14.18943084053789.  
    
    ğŸ§  Fare:
	â€¢	Mean (47.2) > Median (42.0)
	â€¢	Thatâ€™s about a 12% gap
	â€¢	Also note: Max = 250, roughly 5Ã— the mean

		âœ… Interpretation:
			â€¢	Right-skewed â€” a few passengers paid extremely high fares
			â€¢	Likely some first-class / luxury ticket outliers

		ğŸ‘‰ Use RobustScaler for Fare.
	

	ğŸ§  Age:
		â€¢	Mean (26.8) â‰ˆ Median (26.7)
		â€¢	Thatâ€™s less than 0.5% difference
		â€¢	Range is small and reasonable: 0.4 to 53 (no massive outlier)

		âœ… Interpretation:
			â€¢	Fairly symmetric
			â€¢	No heavy tails

		ğŸ‘‰ Use StandardScaler for Age.

    
'''
