'''

------------------------------------------------------------------------------------------------------------
ğŸŒ… Week 4 â€“ Day 2: Data Transformation & Feature Scaling

ğŸ¯ Learning Goals

By the end of today, weâ€™ll be able to:
	1.	Explain why raw numeric features (e.g. Age, Fare, Income, etc.) can distort models if not scaled.
	2.	Apply standardization and normalization using sklearn.preprocessing.
	3.	Understand how scaling affects distance-based models (like KNN or clustering).
	4.	Use Pipeline for safe and reproducible scaling (so you donâ€™t leak data).
	5.	Visualize before/after scaling distributions.
------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------
ğŸ§© Concept Overview

Concept                             What It Means                                           Analogy
-------                             --------------                                          ----------
Standardization                     Transform features so they                              Like shifting and stretching data 
                                    have mean = 0 and std = 1                               onto a â€œcommon measuring stick.â€ 
     
Normalization (Min-Max Scaling)     Scale all values into range [0, 1]                      Like resizing all photos to the same 
                                                                                            frame size before comparison.

RobustScaler                        Uses median & IQR instead of mean/std                   More resistant to outliers.

When to Use                         â€¢ KNN, SVM, Logistic Regression, PCA need scaling
                                    â€¢ Tree-based models (DecisionTree, RandomForest) donâ€™t.
------------------------------------------------------------------------------------------------------------


'''


import pandas as pd
from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
import seaborn as sns

# # 1. Import the dataset
# titdf = pd.read_csv("./data/week4/titanic.csv")
# print("\n\n------ DATA FRAME INFO --------\n\n")
# print(titdf.info(),"\n\n")

# # 2. Clean numeric columns
# num_cols = ['Fare','Age']
# filt_num_data = titdf[num_cols].dropna() # drop NaN for clean comparison

# print("\n\n -- Min Fare --- > ",filt_num_data['Fare'].min())
# print("\n\n -- Max Fare --- > ",filt_num_data['Fare'].max())
# print("\n\n -- mean Fare --- > ",filt_num_data['Fare'].mean())
# print("\n\n -- median Fare --- > ",filt_num_data['Fare'].median())
# print("\n\n -- std deviation in Fare --- > ",filt_num_data['Fare'].std())
# print("\n\n -- Min Age --- > ",filt_num_data['Age'].min())
# print("\n\n -- Max Age --- > ",filt_num_data['Age'].max())
# print("\n\n -- mean Age --- > ",filt_num_data['Age'].mean())
# print("\n\n -- median Age --- > ",filt_num_data['Age'].median())
# print("\n\n -- std deviation in Age --- > ",filt_num_data['Age'].std())

# # 3. Initialize and set different scalers
# std_scaler = StandardScaler()
# minmax_scaler = MinMaxScaler()
# robust_scaler = RobustScaler()

# X_std = std_scaler.fit_transform(filt_num_data)
# X_minmax = minmax_scaler.fit_transform(filt_num_data)
# X_robust = robust_scaler.fit_transform(filt_num_data)

# # 4. Visualize data
# fig, axes = plt.subplots(1, 4, figsize=(15,5))
# sns.kdeplot(filt_num_data['Fare'], ax=axes[0], label='Original')
# sns.kdeplot(X_std[:,1], ax=axes[1], label='Standardized')
# sns.kdeplot(X_minmax[:,1], ax=axes[2], label='Normalized')
# sns.kdeplot(X_robust[:,1], ax=axes[3], label='Robust')
# for ax in axes: ax.legend()
# plt.show()

# # 5. Optimized and Final Setup
# y = titdf.loc[filt_num_data.index, 'Survived']
# X = filt_num_data  # only ['Fare','Age'] with no NaNs

# preprocess = ColumnTransformer(
#     transformers=[
#     ('age_scaler',StandardScaler(),['Age']),
#     ('fare_scaler',RobustScaler(),['Fare'])
# 	],
#     remainder='drop'
# )

# final_pipe = Pipeline([
#     ('prep', preprocess),
#     ('clf',  LogisticRegression(max_iter=1000, random_state=42))
# ])

# final_pipe.fit(X, y)

# # 1ï¸âƒ£ Get only the preprocessing part from the pipeline
# scaler = final_pipe.named_steps['prep']

# # 2ï¸âƒ£ Transform your original X data (Age & Fare)
# X_scaled = scaler.transform(X)

# # 3ï¸âƒ£ Find out what columns came out of the scaler
# col_names = scaler.get_feature_names_out()
# print(col_names)
# # Example output: ['age_scaler__Age', 'fare_scaler__Fare']

# # 4ï¸âƒ£ Turn the scaled NumPy array into a small DataFrame
# X_scaled_df = pd.DataFrame(X_scaled, columns=col_names, index=X.index)

# # 5ï¸âƒ£ Plot using Seaborn
# sns.kdeplot(x=X_scaled_df[col_names[0]], label='Age (scaled)')
# sns.kdeplot(x=X_scaled_df[col_names[1]], label='Fare (scaled)')
# plt.legend()
# plt.title("KDE Plot of Scaled Features")
# plt.show()

'''
	ğŸ§© Mini Challenge

	ğŸ§  Apply StandardScaler and MinMaxScaler to both Age and Fare columns.
	Compare:
		â€¢	Mean & std before and after scaling
		â€¢	Boxplot/KDE shape
		â€¢	How scaling affects outlier influence

	Reflect:
		â€¢	Which scaler best preserved the dataâ€™s shape?
		â€¢	Would you use the same scaler for all models?


'''

df = pd.read_csv("./data/week4/titanic.csv")
# print(df.info())
# print(df.head(3))
# print(df.describe(include='all'))

filnumdata = df[['Age','Fare']].dropna()

print("\n\n Fare mean and std deviation before scaling --> ",filnumdata['Fare'].mean(), "\t --- ", filnumdata['Fare'].std(),"\n\n")
print("\n\n Age mean and std deviation before scaling --> ",filnumdata['Age'].mean(), "\t --- ", filnumdata['Age'].std(),"\n\n")

stdscaler = StandardScaler()
mmscaler = MinMaxScaler()

xstd = stdscaler.fit_transform(filnumdata)

# â€¢	xstd[:, 0] â†’ Age (scaled)
# â€¢	xstd[:, 1] â†’ Fare (scaled)

print("\n\n Fare mean and std deviation after Standard scaling --> ",xstd[:, 1].mean(), "\t --- ", xstd[:, 1].std(),"\n\n")
print("\n\n Age mean and std deviation after Standard scaling --> ",xstd[:, 0].mean(), "\t --- ", xstd[:, 0].std(),"\n\n")

xminmax = mmscaler.fit_transform(filnumdata)

print("\n\n Fare mean and std deviation after MinMax scaling --> ",xminmax[:, 1].mean(), "\t --- ", xminmax[:, 1].std(),"\n\n")
print("\n\n Age mean and std deviation after MinMax scaling --> ",xminmax[:, 0].mean(), "\t --- ", xminmax[:, 0].std(),"\n\n")

# FARE
fig, axes = plt.subplots(1, 3, figsize=(15,5))
sns.kdeplot(filnumdata['Fare'], ax=axes[0], label='Original')
sns.kdeplot(xstd[:,1], ax=axes[1], label='Standardized')
sns.kdeplot(xminmax[:,1], ax=axes[2], label='Normalized')
for ax in axes: ax.legend()
plt.show()

# AGE
fig, axes = plt.subplots(1, 3, figsize=(15,5))
sns.kdeplot(filnumdata['Age'], ax=axes[0], label='Original')
sns.kdeplot(xstd[:,0], ax=axes[1], label='Standardized')
sns.kdeplot(xminmax[:,0], ax=axes[2], label='Normalized')
for ax in axes: ax.legend()
plt.show()


'''
--------------------- RETROSPECTION --------------------

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ğŸ§© 1ï¸âƒ£ Why drop NaN values from numeric columns (instead of imputing)?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When we wrote:
X = df[['Age', 'Fare']].dropna()

we are not saying â€œignore missing values forever.â€
Weâ€™re saying: â€œfor the purpose of demonstrating scaling math (z-score, min-max, etc.), letâ€™s temporarily work with complete data.â€
Because:


Reason                                    	Explanation
------------								--------------
ğŸ§® Scaling formulas need real numbers		The formula for standardization (x - Î¼)/Ïƒ cannot handle NaN â€” even one missing value breaks the computation.

ğŸ§ª Todayâ€™s goal is to see how scaling       We already learned imputation yesterday (Day 1). 
changes numeric distributions, 				Todayâ€™s focus is transformation of numeric values, so we use â€œclean numeric columnsâ€ to avoid mixing concepts.
not how to clean data											

âš™ï¸ Scaling should happen after 				In a real ML workflow, you first impute â†’ then scale. 
imputation in a real pipeline				But for a conceptual demo, we skip the imputation part to directly visualize scaling math.

So dropna() here is a convenience for visualization, not a data-cleaning recommendation.


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ğŸ§© 2ï¸âƒ£ Why only Age and Fare and not Survived?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Because these are the continuous numeric features in the Titanic dataset.
Hereâ€™s how each variable type breaks down:

Column					Type						Nature							Scaling Needed?		 Why / Why Not
--------				-------						----------						---------			 ----------------
Age						Numeric (continuous)		Peopleâ€™s ages (0â€“80s)			âœ… Yes				Different range than other numeric features
Fare					Numeric (continuous)		Ticket prices (0â€“500+)			âœ… Yes				Highly skewed, large variance
Survived				Binary categorical (0/1)	Target label					âŒ No				Itâ€™s the outcome, not an input feature
Embarked, Sex, Cabin	Categorical (non-numeric)	Port, gender, cabin string		âŒ No				Need encoding (like OneHot) before numeric scaling

ğŸ‘‰ So only Age and Fare are â€œtrueâ€ numeric features suitable for scaling.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ğŸ§© 3ï¸âƒ£ Why not impute missing numeric values instead of dropping?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Yes, in production or modeling, you always impute before scaling.

So, the correct â€œreal-worldâ€ order would be:

1ï¸âƒ£ Detect missing values
2ï¸âƒ£ Impute them
3ï¸âƒ£ Then apply scaling

For example:
```python
	from sklearn.impute import SimpleImputer
	from sklearn.preprocessing import StandardScaler
	from sklearn.pipeline import Pipeline

	num_pipe = Pipeline([
		('impute', SimpleImputer(strategy='mean')),
		('scale', StandardScaler())
	])

	X_scaled = num_pipe.fit_transform(df[['Age', 'Fare']])
```
For our learning , weâ€™re intentionally doing scaling on clean data first to:
	â€¢	Understand how scaling behaves mathematically,
	â€¢	Avoid introducing extra noise from imputation (which can change mean/std),
	â€¢	Keep visualizations simple when we plot before/after scaling.
    

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ğŸ§© Explain how StandardScaler, MinMaxScaler and RobustScaler differ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ğŸ§© 1ï¸âƒ£ Why scaling matters in the first place

	Letâ€™s say your dataset has:
	Feature		Typical Range	Example Values
	-------		------------	---------------
	Age			0 â€“ 80			20, 40, 60
	Fare		0 â€“ 500			50, 250, 400

	If you feed this directly into an ML model that uses distance (like KNN, SVM, Logistic Regression),
	the model will think Fare is â€œmore importantâ€ than Age â€” simply because 500 > 80.

	So, scaling brings everything to a common scale before the algorithm computes distances or gradients.

		ğŸ§© When does a model look across columns?

			Only certain models do â€” specifically, models that measure â€œdistanceâ€ or magnitude differences across all features together.

			Letâ€™s take an example to visualize this.

			Suppose you have these two data points (rows in a dataset):
            Passenger	Age	Fare
            --------	--	----âˆš
			A			20	40
			B			40	400
            
            Without scaling:

			The Euclidean distance between A and B is:
						 --------------------------------	
			distance =  âˆš  (20-40)Â² +  (40-400)Â²
            so distance is approx ~ --> 360
            
			Even though Age differs by 20 years, that small 20-point difference is completely dwarfed by the Fare difference (360 units).

			So, when your model looks at the overall distance:
				â€¢	99% of that â€œdistanceâ€ comes from Fare,
				â€¢	Age is almost ignored.
        
        Summary : Ensure that all the columns are scaled properly so that any models that measure distance doesnt provide wrong results.

ğŸ§© 2ï¸âƒ£ Checking how the scalers work with and wihtout outliers 

âš™ï¸ Step 1: The dataset without outliers

	Letâ€™s start simple:

	Passenger	Age			Fare
	---------	---			-----
	A			20			40
	B			30			60
	C			40			80
	D			50			100

	âœ… Everything looks â€œnormal-ish.â€
	We can compute scaling comfortably.

    *** ğŸ“˜ StandardScaler
    
	Z = (x - Î¼)/ğ›”
     
    Mean (Î¼) and Std (Ïƒ):
    Feature		Î¼		Ïƒ
    ------		--		---
	Age			35		12.91
	Fare		70		25.82
    
    Calculate the Z-Score
    Passenger	Age		Age_scaled		Fare	Fare_scaled
    -------		---		---------		---		------------
	A			20		âˆ’1.16			40		âˆ’1.16
	B			30		âˆ’0.39			60		âˆ’0.39
	C			40		+0.39			80		+0.39
	D			50		+1.16			100		+1.16

    Perfectly centered. âœ…

	*** ğŸ“˜ MinMaxScaler

	Formula:
	x` = (x - min) / (max - min)
    
    Passenger	Age		Age_scaled	Fare	Fare_scaled
    --------	---		---------	----	-----------
	A			20		0.0			40		0.0
	B			30		0.33		60		0.33
	C			40		0.67		80		0.67
	D			50		1.0			100		1.0

    All values between 0 and 1, perfect spread. âœ…


	*** ğŸ“˜ RobustScaler

	Uses median (Q2) and IQR = Q3 -	Q1.
    Feature		Median (Q2)		Q1		Q3		IQR (Q3âˆ’Q1)
    -------		----------		---		--		-----------
	Age			35				27.5	45		17.5
	Fare		70				55		85		30

	Formula:
	x` = (x - Q2)/IQR
	Passenger	Age		Age_scaled	Fare	Fare_scaled
    ---------	---		---------	----	------------
	A			20		âˆ’0.86		40		âˆ’1.00
	B			30		âˆ’0.29		60		âˆ’0.33
	C			40		+0.29		80		+0.33
	D			50		+0.86		100		+1.00

	Everythingâ€™s stable and clean. âœ…
    
âš ï¸ Step 2: Now, introduce an outlier

	Letâ€™s add Passenger E with a huge Fare = 1000.
    
	Passenger	Age		Fare		
    --------	---		-----
    A			20		40
	B			30		60
	C			40		80
    D			50		100
	E			50		1000 âŸµ OUTLIER!

    Now letâ€™s see what happens.

	ğŸš¨ StandardScaler with outlier

	New mean and std for Fare:
		â€¢	Mean = (40 + 60 + 80 + 100 + 1000)/5 = 256
		â€¢	Std â‰ˆ 385

	Apply scaling:
    Passenger	Fare	Scaled
    --------	----	-------
	A			40		(40âˆ’256)/385 = âˆ’0.56
	B			60		âˆ’0.51
	C			80		âˆ’0.46	
    D			100		âˆ’0.40
	E			1000	+1.93
    
    ğŸ˜¬ What happened?
	â€¢	Because of one huge value (1000), the mean and std exploded,
	â€¢	making the other 4 â€œnormalâ€ values appear very close to 0, even though they were spread between 40â€“100.
	âš ï¸ StandardScaler gets â€œdistractedâ€ by outliers.
    
    

	ğŸš¨ MinMaxScaler with outlier

	Min = 40, Max = 1000
	Formula â†’ (xâˆ’40)/(1000âˆ’40)
    
    Passenger	Fare	Scaled
    --------	---		------
	A			40		0.0
	B			60		0.02
	C			80		0.04	
    D			100		0.06
	E			1000	1.0

    ğŸ˜¬ Now everything except the outlier is squished into 0â€“0.06 range.
	The model thinks all normal fares are â€œbasically the same.â€
	âš ï¸ MinMaxScaler also suffers badly from outliers.
    
    
    
    âœ… RobustScaler with outlier

	Median (Q2) = 80
	Q1 = 60, Q3 = 100, IQR = 40

	Formula â†’ (xâˆ’80)/40
    
	Passenger	Fare	Scaled
    --------	----	------	
	A			40		âˆ’1.00
	B			60		âˆ’0.50
	C			80		0.00
	D			100		+0.50
	E			1000	+23.00

	âœ¨ Look what happens:
	â€¢	The â€œnormalâ€ range (40â€“100) still stays between âˆ’1 and +1.
	â€¢	The outlier (1000) goes way off (+23) but doesnâ€™t distort everyone elseâ€™s scale.
	RobustScaler keeps the central 50% stable.
    
    Summary for Fare :
    Without outlier:
		StandardScaler:   -1.2  -0.4  0.4  1.2
		MinMaxScaler:     0.0   0.3   0.6  1.0
		RobustScaler:    -0.8  -0.3   0.3  0.8

	With outlier (Fare=1000):
		StandardScaler:   -0.56  -0.51  -0.46  -0.40  +1.93
		MinMaxScaler:     0.00   0.02   0.04   0.06   1.00
		RobustScaler:    -1.00  -0.50   0.00   0.50  +23.00
        
    â€¢	StandardScaler and MinMaxScaler â€œshrinkâ€ everything because the outlier distorts their math.
	â€¢	RobustScaler keeps normal values meaningful â€” it only lets the outlier stretch by itself.
    
    ğŸ§© Step 4: Summary Table
    
    Scaler				Uses			Center		Spread			Outlier Effect		 Good For
    -------				-----			-----		------			--------------		 ----------
	StandardScaler		mean, std		mean=0		std=1			âŒ Large impact		Bell-shaped data
	MinMaxScaler		min, max		0â€“1 range	range-based		âŒ Very sensitive	Bounded features
	RobustScaler		median, IQR		median=0	IQR=1			âœ… Minimal			Outlier-heavy or skewed data

    ************* A quick rule of thumb to select which scaler to be applied 
    
    Relationship between Mean & Median	Interpretation			Implication
    ----------------------------------	--------------			-------------
	Mean â‰ˆ Median						Symmetric (normal-ish)	âœ… StandardScaler fine
	Mean >> Median						Right-skewed			âš ï¸ Use RobustScaler or log-transform
	Mean << Median						Left-skewed (rare)		âš ï¸ Use RobustScaler
    
    In our Titanic dataset
    * Min Fare --- >  13.568895885370686
	* Max Fare --- >  250.0
	* Mean Fare --- >  47.20567742127721
	* Median Fare --- >  42.0616928774529
	* Std deviation in Fare --- >  33.53599558985873
    
	* Min Age --- >  0.4
	* Max Age --- >  53.19996122179421
	* Mean Age --- >  26.7872039001908
	* Median Age --- >  26.671436298735237
	* Std deviation in Age --- >  14.18943084053789.  
    
    ğŸ§  Fare:
	â€¢	Mean (47.2) > Median (42.0)
	â€¢	Thatâ€™s about a 12% gap
	â€¢	Also note: Max = 250, roughly 5Ã— the mean

		âœ… Interpretation:
			â€¢	Right-skewed â€” a few passengers paid extremely high fares
			â€¢	Likely some first-class / luxury ticket outliers

		ğŸ‘‰ Use RobustScaler for Fare.
	

	ğŸ§  Age:
		â€¢	Mean (26.8) â‰ˆ Median (26.7)
		â€¢	Thatâ€™s less than 0.5% difference
		â€¢	Range is small and reasonable: 0.4 to 53 (no massive outlier)

		âœ… Interpretation:
			â€¢	Fairly symmetric
			â€¢	No heavy tails

		ğŸ‘‰ Use StandardScaler for Age.

    
'''


'''
----------------------- DATA VISUALIZATION REF -------------------

SEABORN + SLICING PLAYBOOK (Quick Reference)

import pandas as pd, seaborn as sns, matplotlib.pyplot as plt
df = pd.read_csv(â€./data/week4/titanic.csvâ€)   # has Age, Fare, Embarked, Cabin, Survived, Pclass (if present)

0) First 60 seconds: always do this

	df.info()
	df.head(3)
	df.describe(include=â€œallâ€)
	df[â€˜Embarkedâ€™].value_counts(dropna=False)
    
    

1) Common slices (copy/paste patterns)

numeric only

	num = df[[â€˜Ageâ€™,â€˜Fareâ€™]].dropna()

	boolean filters

	adults = df[df[â€˜Ageâ€™] >= 18]
	cheap = df[df[â€˜Fareâ€™] < 20]
	survivors = df[df[â€˜Survivedâ€™] == 1]
	q_port = df[df[â€˜Embarkedâ€™] == â€˜Qâ€™]

	multiple conditions

	adult_cheap = df[(df[â€˜Ageâ€™]>=18) & (df[â€˜Fareâ€™]<20)]

	select rows/cols

	subset = df.loc[df[â€˜Embarkedâ€™].isin([â€˜Sâ€™,â€˜Câ€™]), [â€˜Ageâ€™,â€˜Fareâ€™,â€˜Embarkedâ€™,â€˜Survivedâ€™]]

	long/melted view for multi-feature plots

	m = df[[â€˜Ageâ€™,â€˜Fareâ€™,â€˜Survivedâ€™]].melt(id_vars=â€˜Survivedâ€™, var_name=â€˜Featureâ€™, value_name=â€˜Valueâ€™)
    
    

2) What plot do I use? (mini decision tree)

	Single numeric: sns.histplot or sns.kdeplot

	Numeric vs numeric: sns.scatterplot (+ hue= a category)

	Numeric by category: sns.boxplot, sns.violinplot, or sns.boxenplot

	Category counts: sns.countplot

	Before/after (e.g., scaling): combine melt + sns.kdeplot or side-by-side boxplot
    
    

3) 10 plug-and-play recipes

	A) Distribution of one numeric

		sns.histplot(data=df, x=â€˜Fareâ€™, bins=30)
		plt.title(â€œFare distributionâ€); plt.show()

	B) KDE with missing dropped

		sns.kdeplot(data=df, x=â€˜Ageâ€™, fill=True)
		plt.title(â€œAge densityâ€); plt.show()

	C) Numeric by category (outliers pop!)

		sns.boxplot(data=df, x=â€˜Embarkedâ€™, y=â€˜Fareâ€™)
		sns.stripplot(data=df, x=â€˜Embarkedâ€™, y=â€˜Fareâ€™, alpha=0.5, color=â€˜kâ€™, size=2)
		plt.title(â€œFare by Embarkedâ€); plt.show()

	D) Two numerics + category hue

		sns.scatterplot(data=df, x=â€˜Ageâ€™, y=â€˜Fareâ€™, hue=â€˜Survivedâ€™, alpha=0.7)
		plt.title(â€œAge vs Fare colored by Survivedâ€); plt.show()

	E) Compare scaled vs unscaled (your Day-2 use case)

		suppose you already made X_scaled_df with columns [â€˜age_scaler__Ageâ€™,â€˜fare_scaler__Fareâ€™]

		orig = df[[â€˜Ageâ€™,â€˜Fareâ€™]].dropna().assign(Source=â€˜Originalâ€™)
		scaled = pd.DataFrame(X_scaled_df.values, columns=[â€˜Ageâ€™,â€˜Fareâ€™]).assign(Source=â€˜Scaledâ€™)
		both = pd.concat([orig, scaled], ignore_index=True)
		sns.kdeplot(data=both, x=â€˜Ageâ€™, hue=â€˜Sourceâ€™)
		plt.title(â€œAge: original vs scaledâ€); plt.show()
		sns.kdeplot(data=both, x=â€˜Fareâ€™, hue=â€˜Sourceâ€™)
		plt.title(â€œFare: original vs scaledâ€); plt.show()

	F) Facets: same plot split by a category

		g = sns.FacetGrid(df, col=â€˜Embarkedâ€™, col_wrap=3, height=3, sharex=False, sharey=False)
		g.map_dataframe(sns.histplot, x=â€˜Fareâ€™, bins=20)
		g.fig.suptitle(â€œFare by Embarkedâ€, y=1.03); plt.show()

	G) Before/after outlier capping

		after you create df[â€˜Fare_cappedâ€™] = df[â€˜Fareâ€™].clip(lower, upper)

		m = df[[â€˜Fareâ€™, â€˜Fare_cappedâ€™]].melt(var_name=â€˜Versionâ€™, value_name=â€˜Valueâ€™)
		sns.boxenplot(data=m, x=â€˜Versionâ€™, y=â€˜Valueâ€™)
		plt.title(â€œFare: original vs cappedâ€); plt.show()

	H) Quick missingness bar

		na = df.isna().sum().reset_index().rename(columns={â€˜indexâ€™:â€˜colâ€™,0:â€˜naâ€™})
		sns.barplot(data=na, x=â€˜naâ€™, y=â€˜colâ€™)
		plt.title(â€œMissing values by columnâ€); plt.show()

	I) Log transform for skew (visual)

		import numpy as np
		sns.kdeplot(x=np.log1p(df[â€˜Fareâ€™]), fill=True)
		plt.title(â€œlog1p(Fare)â€); plt.show()

	J) Heatmap of numeric correlations

		sns.heatmap(df[[â€˜Ageâ€™,â€˜Fareâ€™,â€˜Survivedâ€™]].corr(), annot=True, fmt=â€™.2fâ€™, cmap=â€˜vlagâ€™)
		plt.title(â€œCorrelation (numeric)â€); plt.show()
		
    

4) Typical â€œwhy is my plot blank / error?â€ fixes

	NaNs: sns.*plot silently drops them. Use .dropna() on the columns you plot.

	Wrong axis: For arrays from scalers (NumPy), use correct column index (X_std[:, 0] for Age if you fit on [Age, Fare]).

	Dtypes: Categories must be object/category; numbers should be float/int.

	df[â€˜Survivedâ€™] = df[â€˜Survivedâ€™].astype(â€˜categoryâ€™)
	df[â€˜Embarkedâ€™] = df[â€˜Embarkedâ€™].astype(â€˜categoryâ€™)

	Melt vs wide: If you want one axis to say â€œFeatureâ€ and another â€œValueâ€, you must melt first.

5) Minimal styling that just works

	sns.set_theme(context=â€œnotebookâ€, style=â€œwhitegridâ€)
	plt.tight_layout()

6) Reusable helpers

	def kde_compare(dfA, dfB, col, labels=(â€œAâ€,â€œBâ€), title=None):
	sns.kdeplot(dfA[col].dropna(), label=labels[0])
	sns.kdeplot(dfB[col].dropna(), label=labels[1])
	plt.legend()
	if title: plt.title(title)
	plt.show()

	def box_by_cat(df, y, x, title=None):
	sns.boxplot(data=df, x=x, y=y)
	sns.stripplot(data=df, x=x, y=y, color=â€˜kâ€™, alpha=0.4, size=2)
	if title: plt.title(title)
	plt.show()
'''