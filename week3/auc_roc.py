#Week 3 - Day 5
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.datasets as skd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler,label_binarize
from sklearn.metrics import accuracy_score,classification_report,ConfusionMatrixDisplay,roc_auc_score,roc_curve

iris = skd.load_iris()
fdf = pd.DataFrame(iris.data, columns= iris.feature_names)
trgt = pd.Series(iris.target)

X_train,X_test,Y_train,Y_test = train_test_split(fdf,trgt,train_size=0.2,random_state=42,stratify=trgt)

lrpipleline = Pipeline([
    ("sclr",StandardScaler()),
    ("lrmdl",LogisticRegression(max_iter=1000))
])
lrpipleline.fit(X_train,Y_train)

#Binarize labels (convert 3-class labels into one-hot form)
Y_test_bin = label_binarize(Y_test,classes=[0,1,2])
n_classes = Y_test_bin.shape[1]

# Get model probabilities
# ROC requires probabilities, not just predictions
lrp_y_pred = lrpipleline.predict_proba(X_test)  #Proba is available for Logistic regression

#Compute ROC and AUC for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    fpr[i],tpr[i],_ = roc_curve(Y_test_bin[:,i],lrp_y_pred[:,i])
    roc_auc[i] = roc_auc_score(Y_test_bin[:,i],lrp_y_pred[:,i])

plt.figure(figsize=(8,6))
for i, class_name in enumerate(iris.target_names):
    plt.plot(fpr[i], tpr[i], label=f"{class_name} (AUC = {roc_auc[i]:.2f})")

plt.plot([0,1], [0,1], "k--")   # diagonal = random guessing
plt.xlabel("False Positive Rate")

plt.ylabel("True Positive Rate")
plt.title("One-vs-Rest ROC Curves (Logistic Regression)")
plt.legend(loc="lower right")
plt.show()


'''
ğŸ”¹ Why Accuracy Isnâ€™t Always Enough
	â€¢	Accuracy = correct predictions/total predictions.
	â€¢	Works fine for balanced datasets like Iris.
	â€¢	But in imbalanced data (e.g., 95% healthy patients, 5% sick patients):
	â€¢	A model that predicts â€œhealthyâ€ for everyone gets 95% accuracy â€” but is useless.

ğŸ”¹  Precision, Recall, F1-Score

Letâ€™s break them down :
	â€¢	Precision â†’ â€œOf all flowers I said were Virginica, how many really were Virginica?â€
         - Formula: 
                Precision} = TP/(TP + FP) --> TP (True Prediction), FP (False Prediction)
         - Translation: How careful the model is when it says Yes.

	â€¢	Recall â†’ â€œOf all the Virginica flowers, how many did I actually catch?â€
	    - Formula: 
            Recall = TP/(TP + FN). --> FN (False Negatives)
	    - Translation: How good the model is at catching everything.

	â€¢	F1-Score â†’ Balance between precision and recall.
	    - Formula:
            F1 = 2 * (Precision Ã— Recall)/(Precision + Recall)
	    - Translation: The teacherâ€™s â€œoverall gradeâ€ when you need both neatness and speed.

    Precision vs Recall with an exmaple      

    ğŸ”¹ Imagine a Medical Test Example (Simple Case)

    Letâ€™s say weâ€™re testing 100 people for a disease:
        â€¢	10 people are actually sick.
        â€¢	90 people are actually healthy.

    Case 1: Precision
    -----------------
    ğŸ‘‰ Question: â€œWhen the test says someone is sick, how often is it right?â€
        â€¢	If the test says 20 people are sickâ€¦
        â€¢	8 are actually sick (true positives)
        â€¢	12 are actually healthy (false positives)

    Precision= TP/ (TP + FP)  = 8/(8+12) = 0.40

    â¡ï¸ Only 40% of the positive results were correct â†’ the test gives too many false alarms.

    
    Case 2: Recall
    --------------
    ğŸ‘‰ Question: â€œOf all the sick people, how many did the test catch?â€
        â€¢	Out of 10 sick people, the test correctly identified 8.
        â€¢	2 sick people slipped through (false negatives).

    Recall= TP/(TP + FN) = 8/(8+2) = 0.80

    â¡ï¸ The test caught 80% of the sick people, but it missed 20%.
   

    ğŸ”¹ Key Difference
        â€¢	Precision cares about the quality of positive predictions.
            â€¢	â€œWhen I say YES, how often am I right?â€
        â€¢	Recall cares about the coverage of actual positives.
            â€¢	â€œDid I catch everyone whoâ€™s actually YES?â€
    
    ğŸ”¹ When do they differ?
        1.	Spam filter:
            â€¢	Precision high â†’ almost everything marked spam really is spam (few false alarms).
            â€¢	Recall low â†’ some spam still sneaks into inbox.
        2.	Medical test:
            â€¢	Recall high â†’ catches nearly all sick patients (few misses).
            â€¢	Precision low â†’ some healthy patients are incorrectly flagged.

    So they measure different trade-offs.
    
    âœ… Think of it this way:
        â€¢	Precision = trust â†’ Can I trust the â€œYESâ€ predictions?
        â€¢	Recall = coverage â†’ Did I find all the actual â€œYESâ€ cases?

'''