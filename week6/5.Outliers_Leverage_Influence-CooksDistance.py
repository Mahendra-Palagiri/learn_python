'''Week 6 ¬∑ Day 5 ‚Äî Outliers, Leverage, Influence (Cook‚Äôs Distance)

Topics
	‚Ä¢	What residual ‚Äúoutliers‚Äù are vs what ‚Äúinfluential‚Äù points are
	‚Ä¢	Leverage: unusual X values (feature-space extremes)
	‚Ä¢	Influence: points that can noticeably change our fitted line/model
	‚Ä¢	Cook‚Äôs Distance, leverage (hat values), studentized residuals
	‚Ä¢	How we decide what to investigate (not auto-delete rows)

üéØ Learning Goals

By the end of Day 5, we will be able to:
	‚Ä¢	Explain the difference between:
	‚Ä¢	large residuals (badly predicted)
	‚Ä¢	high leverage (unusual inputs)
	‚Ä¢	high influence (changes the model)
	‚Ä¢	Compute and visualize:
	‚Ä¢	Cook‚Äôs Distance
	‚Ä¢	leverage (hat values)
	‚Ä¢	studentized residuals
	‚Ä¢	Identify the top influential rows and inspect them safely
	‚Ä¢	Decide what we would try next (robust models, transforms, feature work), without blindly removing data

'''

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing

# ==========================================================
# Step 1 ‚Äî Fit an OLS model on our training Set
# ==========================================================

chdf = fetch_california_housing(as_frame=True).frame
'''
print("\n==============================================================================")
print(chdf.info())
'''

target='MedHouseVal'
X = chdf.drop(columns=[target])
Y = chdf[target]

X_trainval,X_test,Y_trainval,Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)
X_train,X_val,Y_train,Y_val = train_test_split(X_trainval,Y_trainval,test_size=0.2,random_state=42)

X_train_sm = sm.add_constant(X_train)
ols = sm.OLS(Y_train,X_train_sm).fit()

'''
print("\n==============================================================================")
print("R2:",ols.rsquared)
print("Adj R2:",ols.rsquared_adj)

R2: 0.6131488911003316
Adj R2: 0.6129144358828167
'''


# ==========================================================
# Step 2 ‚Äî Compute influence metrics (Cook‚Äôs D, leverage, studentized residuals)
# ==========================================================

influence = ols.get_influence()

# Core diagnostics
cooks_d = influence.cooks_distance[0]          # array
leverage = influence.hat_matrix_diag           # array
stud_resid = influence.resid_studentized_internal  # array


'''
print("\n==============================================================================")
print("\nDiagnostics arrays:")
print("Cook's D shape:", cooks_d.shape)
print("Leverage shape:", leverage.shape)
print("Studentized residual shape:", stud_resid.shape)

Diagnostics arrays:
Cook's D shape: (13209,)
Leverage shape: (13209,)
Studentized residual shape: (13209,)

What each one means (simple)
	‚Ä¢	Studentized residual: ‚Äúhow extreme is the error‚Äù after scaling (like a normalized residual)
	‚Ä¢	Leverage: ‚Äúhow unusual is this X row compared to others‚Äù
	‚Ä¢	Cook‚Äôs D: ‚Äúif we removed this row, how much would the model change?‚Äù

'''

# ==========================================================
# Step 3 ‚Äî Plot Cook‚Äôs Distance (who are the top influencers?)
# ==========================================================
plt.figure()
plt.stem(np.arange(len(cooks_d)), cooks_d)  # stems help us see spikes
plt.xlabel("Row index (train)")
plt.ylabel("Cook's Distance")
plt.title("Cook's Distance ‚Äî Training Set")
plt.savefig("week6/5.1CooksDistancePlot.png")
# plt.show()


# ~~~ Now we'll find the top 10 influential points:
top_k = 10
top_idx = np.argsort(cooks_d)[-top_k:][::-1]

'''
print("\n==============================================================================")
print("Top influential points by Cook's D:")
for rank, idx in enumerate(top_idx, 1):
    print(f"{rank:2d}) idx={idx}  CookD={cooks_d[idx]:.6f}  leverage={leverage[idx]:.6f}  stud_resid={stud_resid[idx]:.3f}")


Top influential points by Cook's D:
 1) idx=2757  CookD=2.001294  leverage=0.576413  stud_resid=3.638
 2) idx=2580  CookD=1.904147  leverage=0.403434  stud_resid=5.034
 3) idx=225  CookD=0.484712  leverage=0.117223  stud_resid=-5.732
 4) idx=10876  CookD=0.135771  leverage=0.067223  stud_resid=4.118
 5) idx=3798  CookD=0.123410  leverage=0.054898  stud_resid=-4.373
 6) idx=9659  CookD=0.051227  leverage=0.051593  stud_resid=2.911
 7) idx=1080  CookD=0.037541  leverage=0.004830  stud_resid=-8.343
 8) idx=4023  CookD=0.036925  leverage=0.033731  stud_resid=-3.085
 9) idx=10332  CookD=0.036688  leverage=0.033101  stud_resid=-3.106
10) idx=2308  CookD=0.026171  leverage=0.047587  stud_resid=-2.171
'''

# ==========================================================
# Step 4 ‚Äî Inspect the top influencer points
# ==========================================================
top_rows = X_train.iloc[top_idx].copy()
top_rows["y_train"] = Y_train.iloc[top_idx].values
top_rows["CookD"] = cooks_d[top_idx]
top_rows["Leverage"] = leverage[top_idx]
top_rows["StudResid"] = stud_resid[top_idx]

'''
print("\n==============================================================================")
print("\nTop rows (first 5 shown):")
print(top_rows.head())


Top rows (first 5 shown):
       MedInc  HouseAge   AveRooms  AveBedrms  Population  ...  Longitude  y_train     CookD  Leverage  StudResid
3364   5.5179      36.0   5.142857   1.142857      4198.0  ...    -120.51    0.675  2.001294  0.576413   3.638153
16669  4.2639      46.0   9.076923   1.307692      6532.0  ...    -120.70    3.500  1.904147  0.403434   5.034018
11862  2.6250      25.0  59.875000  15.312500        28.0  ...    -121.25    0.675  0.484712  0.117223  -5.731689
1913   4.0714      19.0  61.812500  11.000000       112.0  ...    -120.06    4.375  0.135771  0.067223   4.117704
1102   2.4028      17.0  31.777778   9.703704        47.0  ...    -121.54    0.675  0.123410  0.054898  -4.372778

[5 rows x 12 columns]

What we‚Äôre looking for
	‚Ä¢	Extremely high/low MedInc, rooms/bedrooms ratios, unusual geo coords, etc.
	‚Ä¢	Rows that look ‚Äúweird‚Äù (possible data artifacts) vs ‚Äúrare but valid‚Äù cases
'''

# ==========================================================
# Step 5 ‚Äî A useful combined diagnostic plot
# ==========================================================
plt.figure()
plt.scatter(leverage, stud_resid, s=20)
plt.xlabel("Leverage")
plt.ylabel("Studentized Residual")
plt.title("Leverage vs Studentized Residual (Training)")
plt.axhline(0)
plt.savefig("week6/5.2Leverage_StudentizedResidual.png")
# plt.show()

'''
Interpretation:
	‚Ä¢	high leverage + large |studentized residual| are the most suspicious
	‚Ä¢	Cook‚Äôs D helps quantify overall influence
'''


# ==========================================================
# RETROSPECTION
# ==========================================================
'''
Understand more about the metrics
        * How its calculated
        * What is the expected value
        * What values are considered as deviations and how to measure. (Close, too big, too small)

    A) Stundentized Residual
        
        Let‚Äôs make it concrete with one row

        Pick any index i (we‚Äôll use the ‚Äútop Cook‚Äôs D‚Äù indices soon). For that i:

        A) Studentized residual (error extremeness)
            ‚Ä¢	residual = pred - actual
            ‚Ä¢	studentized residual = residual divided by an estimate of its standard deviation

        So it‚Äôs like a z-score for errors:
            ‚Ä¢	stud_resid = 0 ‚Üí prediction is basically on target
            ‚Ä¢	stud_resid = 2.5 ‚Üí about 2.5 ‚Äústandard error units‚Äù off (pretty large)
            ‚Ä¢	stud_resid = -3.0 ‚Üí very underpredicted

        Rule of thumb:
            ‚Ä¢	|stud_resid| > 2 is worth looking at
            ‚Ä¢	|stud_resid| > 3 is very suspicious

    B) Leverage (X unusualness)

        Leverage is: ‚Äúis this row‚Äôs X far from the center of the X cloud?‚Äù
            ‚Ä¢	high leverage means the feature combination is rare/extreme (unusual latitude/longitude, extreme MedInc, extreme room ratios, etc.)

        Rule of thumb:
            ‚Ä¢	average leverage ‚âà (p + 1) / n
        where p = number of features, +1 for intercept
        Here p=8 (we have 8 features in our california housing dataset),
        so average ‚âà 9/13209 ‚âà 0.00068
            ‚Ä¢	values several times larger than that (like >0.005 or >0.01) are ‚Äúhigh‚Äù relative to the average

    C) Cook‚Äôs D (overall influence)

        Cook‚Äôs D combines both:
            ‚Ä¢	high leverage (unusual X)
            ‚Ä¢	and/or high residual (big error)

        Cook‚Äôs D answers:

        ‚ÄúIf we remove this row and refit, how much would the fitted model change overall?‚Äù

        Rule of thumb:
            ‚Ä¢	CookD > 4/n is often used as a ‚Äúflag‚Äù
        here 4/13209 ‚âà 0.00030
        (not a hard rule ‚Äî just a trigger to inspect)

‚∏ª

    Why these three are different (simple scenarios)

    Scenario 1: Big error but normal X
        ‚Ä¢	high |stud_resid|
        ‚Ä¢	low/moderate leverage
        ‚Ä¢	CookD might be moderate
    Meaning: the model struggles on this case, but removing it won‚Äôt change the model much.

    Scenario 2: Weird X but model predicts it well
        ‚Ä¢	low |stud_resid|
        ‚Ä¢	high leverage
        ‚Ä¢	CookD can still be noticeable
    Meaning: it‚Äôs an extreme input. Even if it fits well, it can ‚Äúanchor‚Äù the line.

    Scenario 3: Weird X AND big error (most dangerous)
        ‚Ä¢	high |stud_resid|
        ‚Ä¢	high leverage
        ‚Ä¢	high CookD
    Meaning: this row can pull the model and distort coefficients.

‚∏ª

    Top influential points by Cook's D:
    1) idx=2757  CookD=2.001294  leverage=0.576413  stud_resid=3.638
    2) idx=2580  CookD=1.904147  leverage=0.403434  stud_resid=5.034
    3) idx=225  CookD=0.484712  leverage=0.117223  stud_resid=-5.732
    4) idx=10876  CookD=0.135771  leverage=0.067223  stud_resid=4.118
    5) idx=3798  CookD=0.123410  leverage=0.054898  stud_resid=-4.373
    6) idx=9659  CookD=0.051227  leverage=0.051593  stud_resid=2.911
    7) idx=1080  CookD=0.037541  leverage=0.004830  stud_resid=-8.343
    8) idx=4023  CookD=0.036925  leverage=0.033731  stud_resid=-3.085
    9) idx=10332  CookD=0.036688  leverage=0.033101  stud_resid=-3.106
    10) idx=2308  CookD=0.026171  leverage=0.047587  stud_resid=-2.171

    For each top row, we‚Äôll classify it like:
	‚Ä¢	‚ÄúHigh CookD because leverage is huge‚Äù
	‚Ä¢	‚ÄúHigh CookD because residual is huge‚Äù
	‚Ä¢	‚ÄúBoth ‚Üí very influential‚Äù

    We‚Äôll read each row like:
	‚Ä¢	Leverage: ‚Äúhow unusual are the inputs (X)?‚Äù
	‚Ä¢	Studentized residual: ‚Äúhow big is the error (standardized)?‚Äù
	‚Ä¢	Cook‚Äôs D: ‚Äúoverall influence = (unusual X) + (big error) ‚Üí how much the model can change if we remove it‚Äù

    1) idx=2757 ‚Äî CookD=2.00, leverage=0.576, stud_resid=3.638

        Diagnosis: extreme leverage + large error ‚Üí very influential
        This is the most dangerous type:
            ‚Ä¢	unusual X and wrong prediction
            ‚Ä¢	can pull coefficients strongly

    2) idx=2580 ‚Äî CookD=1.90, leverage=0.403, stud_resid=5.034

        Diagnosis: extreme leverage + very large error ‚Üí very influential
        Same as #1, even more error.

    ‚úÖ These two are ‚Äúred alert‚Äù points.

    ‚∏ª

    3) idx=225 ‚Äî CookD=0.485, leverage=0.117, stud_resid=-5.732

        Diagnosis: high leverage + very large error (underprediction)
        Less leverage than #1/#2 but still insanely high vs normal.

    ‚∏ª

    4) idx=10876 ‚Äî CookD=0.136, leverage=0.067, stud_resid=4.118

        Diagnosis: moderately high leverage + large error
        Still influential.

    5) idx=3798 ‚Äî CookD=0.123, leverage=0.055, stud_resid=-4.373

     Diagnosis: moderately high leverage + large error

    6) idx=9659 ‚Äî CookD=0.051, leverage=0.052, stud_resid=2.911

        Diagnosis: high leverage + moderate-to-large error
        Influential mainly because leverage is high.

    ‚∏ª

    7) idx=1080 ‚Äî CookD=0.0375, leverage=0.00483, stud_resid=-8.343

        Diagnosis: huge error but leverage is not extreme (still > average, but not crazy).
        This is the ‚Äúbig residual‚Äù type:
            ‚Ä¢	model is very wrong for this point
            ‚Ä¢	but because X isn‚Äôt super unusual, removing it won‚Äôt rotate the whole model as much as #1/#2.

    Still worth inspecting because |stud_resid| = 8.3 is enormous.

    ‚∏ª

    8) idx=4023 ‚Äî CookD=0.0369, leverage=0.0337, stud_resid=-3.085

        Diagnosis: moderate leverage + large error

    9) idx=10332 ‚Äî CookD=0.0367, leverage=0.0331, stud_resid=-3.106

        Diagnosis: moderate leverage + large error

    10) idx=2308 ‚Äî CookD=0.0262, leverage=0.0476, stud_resid=-2.171

        Diagnosis: higher leverage + moderate error
        Influence mostly from leverage.

    ‚∏ª

    Summary diagnosis (what we learned today)

    What‚Äôs driving Cook‚Äôs D in our data?

    Mostly leverage.

    Look at #1 and #2:
        ‚Ä¢	leverage 0.576 and 0.403 are unbelievably high.
    That‚Äôs why CookD is huge.

    What does that mean?

    We likely have a handful of training rows with extreme feature combinations that are not typical of the dataset. 
    They can distort the fitted coefficients

 np.argsort(cooks_d)[-top_k:][::-1] -->  what we are doing here

    What we want

        We want the indices (row numbers) of the top_k largest Cook‚Äôs Distance values ‚Äî i.e., the most influential points.

        cooks_d is an array like:
            ‚Ä¢	cooks_d[0] = Cook‚Äôs D for row 0
            ‚Ä¢	cooks_d[1] = Cook‚Äôs D for row 1
            ‚Ä¢	‚Ä¶
        So we need the row indices of the biggest values.

    ‚∏ª

    Step 1: np.argsort(cooks_d)

    argsort does not return sorted values.
    It returns the indices that would sort the array.

    example:
        cooks_d = [0.2, 0.05, 1.3, 0.4]
        np.argsort(cooks_d)  -> [1, 0, 3, 2]

    Why?
	‚Ä¢	cooks_d[1] = 0.05 (smallest)
	‚Ä¢	cooks_d[0] = 0.2
	‚Ä¢	cooks_d[3] = 0.4
	‚Ä¢	cooks_d[2] = 1.3 (largest)

    So after argsort, we have indices in ascending order of Cook‚Äôs D.

    ‚∏ª

    Step 2: [-top_k:]

    This slices the last top_k indices from that sorted list.

    Since the list is in ascending order:
        ‚Ä¢	the last top_k correspond to the largest Cook‚Äôs D values

    So now we have ‚Äúthe indices of the biggest Cook‚Äôs D rows‚Äù, but still in ascending order.

    ‚∏ª

    Step 3: [::-1]

    This reverses the slice.

    So now we have the indices in descending order:
        ‚Ä¢	biggest Cook‚Äôs D first
        ‚Ä¢	then second biggest
        ‚Ä¢	etc.

    ‚∏ª

    Final result

    top_idx becomes something like:
    [2757, 2580, 225, 10876, ...]

    These are the row indices in the training set with the highest influence.
    
'''