'''Week 6 ¬∑ Day 4 ‚Äî Regression Diagnostics (Residuals, Non-Linearity, Heteroskedasticity)

Topics
	‚Ä¢	What residuals are and why diagnostics matter
	‚Ä¢	Residual plots to detect:
	‚Ä¢	non-linearity (curvature)
	‚Ä¢	heteroskedasticity (fan / funnel shape)
	‚Ä¢	outliers (extreme residuals)
	‚Ä¢	‚ÄúGood vs suspicious‚Äù diagnostic patterns
	‚Ä¢	Quick tests (optional): Breusch‚ÄìPagan for heteroskedasticity

üéØ Learning Goals

By the end of Day 4, we will be able to:
	‚Ä¢	Generate residual diagnostics plots correctly
	‚Ä¢	Recognize common failure patterns (curve, fan, clusters)
	‚Ä¢	Explain what each pattern suggests we should try next
	‚Ä¢	Understand why ‚Äúpretty R^2‚Äù is not enough

'''

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm


from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from statsmodels.stats.diagnostic import het_breuschpagan


# ==========================================================
# Step 1 ‚Äî Fit a model we will diagnose
# ==========================================================
# 1Ô∏è‚É£ Load and Split data
chdf = fetch_california_housing(as_frame=True).frame

# print(chdf.info())

target = 'MedHouseVal'
X = chdf.drop(columns=[target])
Y = chdf[target]

X_trainval,X_test,Y_trainval,Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)
X_train,X_val,Y_train,Y_val = train_test_split(X_trainval,Y_trainval,test_size=0.2,random_state=42)

# 2Ô∏è‚É£ Fit Model
lr = LinearRegression()
lr.fit(X_train,Y_train)

# 3Ô∏è‚É£ Predict based on validation dataset
yval_pred = lr.predict(X_val)
residuals = yval_pred-Y_val
rmse = np.sqrt(mean_squared_error(Y_val,yval_pred))

print("\n==============================================================================")
print("Validation RMSE:",rmse,"\n")

# ==========================================================
# Step 2 ‚Äî Diagnostic plot set
# ==========================================================

# Plot A: Residuals vs Predicted
plt.figure()
plt.scatter(yval_pred,residuals)
plt.axhline(0)
plt.xlabel("Predicted Y (Val)")
plt.ylabel("Residucals (Val)")
plt.title("Predicted vs Residuals -- Validation Set")
# plt.show()

'''
What we want: random cloud centered around 0
Suspicious: curve or fan shape
'''

# Plot B: Residuals vs Actual. (Useful when predicted values are clipped or bunched.)
plt.figure()
plt.scatter(Y_val,residuals)
plt.axhline(0)
plt.xlabel("Actual Y (Val)")
plt.ylabel("Residuals (Val)")
plt.title("Acutal vs Residual  --> Validation Set")
# plt.show()


# Plot C: Predicted vs Actual with diagonal
plt.figure()
plt.scatter(Y_val,yval_pred)
min_y = min(Y_val.min(), yval_pred.min())
max_y = max(Y_val.max(), yval_pred.max())
plt.plot([min_y,max_y],[min_y,max_y])
plt.xlabel("Actual (Val)")
plt.ylabel("Predicted (Val)")
plt.title("Actual vs Predicted --> Validation Set")
# plt.show()

# Plot D: Residual histogram (distribution sanity)
plt.figure()
plt.hist(residuals,bins=50)
plt.xlabel("Residual")
plt.ylabel("Count")
plt.title("Residual Distribution --> Validation Set")
# plt.show()


# ==========================================================
# Step 3 :: Diagnostic plot set (ALL IN ONE FIGURE)
# ==========================================================

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot A: Residuals vs Predicted
ax = axes[0, 0]
ax.scatter(yval_pred, residuals, alpha=0.6)
ax.axhline(0)
ax.set_xlabel("Predicted Y (Val)")
ax.set_ylabel("Residuals (Val)")
ax.set_title("Predicted vs Residuals ‚Äî Validation Set")

# Plot B: Residuals vs Actual
ax = axes[0, 1]
ax.scatter(Y_val, residuals, alpha=0.6)
ax.axhline(0)
ax.set_xlabel("Actual Y (Val)")
ax.set_ylabel("Residuals (Val)")
ax.set_title("Actual vs Residuals ‚Äî Validation Set")

# Plot C: Predicted vs Actual with diagonal
ax = axes[1, 0]
ax.scatter(Y_val, yval_pred, alpha=0.6)
min_y = min(Y_val.min(), yval_pred.min())
max_y = max(Y_val.max(), yval_pred.max())
ax.plot([min_y, max_y], [min_y, max_y])
ax.set_xlabel("Actual (Val)")
ax.set_ylabel("Predicted (Val)")
ax.set_title("Actual vs Predicted ‚Äî Validation Set")

# Plot D: Residual histogram
ax = axes[1, 1]
ax.hist(residuals, bins=50)
ax.set_xlabel("Residual")
ax.set_ylabel("Count")
ax.set_title("Residual Distribution ‚Äî Validation Set")

plt.tight_layout()
fig.savefig("week6/4.1Regression_diagnostics.png", dpi=200, bbox_inches="tight")
# plt.show()

"""
![Diagnostics](week6_day4_regression_diagnostics.png)
"""

# ==========================================================
# Step 4 :: Breusch‚ÄìPagan test for heteroskedasticity
# ==========================================================

X_val_sm = sm.add_constant(X_val)
ols = sm.OLS(Y_val,X_val_sm).fit()

bp = het_breuschpagan(ols.resid,X_val_sm)
labels = ["LM Stat", "LM p-value", "F Stat", "F p-value"]

print("\nBreusch‚ÄìPagan test:")
for name, val in zip(labels, bp):
    print(name, ":", val)

'''
Breusch‚ÄìPagan test:
LM Stat : 300.6642950412045
LM p-value : 2.9735644217256013e-60
F Stat : 41.234070953073186
F p-value : 3.1039460445456923e-63

Interpretation (simple):
	‚Ä¢	low p-value suggests heteroskedasticity is present
'''

''' ---- RETROSPECTION -----
What does these patterns mean   (4.1 Regression_diagnositics.png (simple mapping)

**** If we see a ‚Äúcurve‚Äù in residuals vs predicted

        Likely:
            ‚Ä¢	non-linear relationship we didn‚Äôt model
        Next try:
            ‚Ä¢	polynomial features
            ‚Ä¢	interactions
            ‚Ä¢	log transforms

**** If we see a ‚Äúfan‚Äù / ‚Äúfunnel‚Äù

        Likely:
            ‚Ä¢	heteroskedasticity (variance changes with prediction level)
        Next try:
            ‚Ä¢	transform y (like log)
            ‚Ä¢	robust regression / robust SE (later)
            ‚Ä¢	different model class

**** If we see extreme points far away

        Likely:
            ‚Ä¢	outliers or influential points
        Next try:
            ‚Ä¢	examine those rows
            ‚Ä¢	Cook‚Äôs distance (Day 5)


# EXPLANATION ON THE GRAPH (4.1Regression_diagnostics.png)
1) Predicted vs Residuals (top-left): why that sharp diagonal ‚Äúedge‚Äù?

We defined residuals as:

residual = yÀÜ - y

In the California Housing dataset, the target MedHouseVal is capped at 5.0 (in $100k units). So:

y <= 5

That means residuals must satisfy:

residual = yÀÜ - y >= y - 5

So there‚Äôs a hard lower boundary line:

residual = yÀÜ - 5

‚úÖ That is exactly the sharp diagonal ‚Äúfloor‚Äù you‚Äôre seeing.
Points can‚Äôt fall below it because the true y can‚Äôt exceed 5.

Interpretation: This is not ‚Äúa model bug.‚Äù It‚Äôs a dataset label cap creating a geometric constraint in residual space.

‚∏ª

2) Actual vs Residuals (top-right): why the vertical band at Actual = 5?

Same reason: many samples have Actual y = 5 (hit the cap).
So we see a vertical wall at x = 5.

Also notice the residuals near actual=5 spread a lot ‚Äî that‚Äôs telling us:
	‚Ä¢	for capped cases, the model can predict above/below 5, but the true label is stuck at 5
	‚Ä¢	residuals there can look systematically biased because the data is censored.

‚∏ª

3) Actual vs Predicted (bottom-left): why the tall vertical column at Actual = 5?

Again: lots of points have Actual=5, so we get a vertical stack.

Also, notice we sometimes predict above 5 (up to ~7+). That‚Äôs normal for plain linear regression because it doesn‚Äôt ‚Äúknow‚Äù the label cap. It will happily extrapolate beyond 5.

‚∏ª

4) Residual distribution (bottom-right): what do we see?

The histogram is:
	‚Ä¢	roughly centered near 0 (good)
	‚Ä¢	but not perfectly symmetric, and there‚Äôs a heavier tail (common in real-world regression)

With this dataset, we should expect residual weirdness because:
	‚Ä¢	the target is censored at 5 (so errors can‚Äôt behave like a clean bell curve),
	‚Ä¢	and the true relationship is not perfectly linear.

‚∏ª

What does this mean for Day 4 diagnostics?

What looks ‚Äúgood‚Äù
	‚Ä¢	We do see a reasonable diagonal trend in Actual vs Predicted.
	‚Ä¢	Residuals are broadly around 0 for a big chunk of the data.

What looks ‚Äúsuspicious / informative‚Äù
	‚Ä¢	The cap-driven boundary (diagonal floor + vertical wall) is telling us our target is censored.
	‚Ä¢	The spread of residuals varies with prediction/actual in places ‚Üí likely heteroskedasticity (variance not constant).
	‚Ä¢	There may also be some non-linearity (common with housing).

‚∏ª

What we should do next (our Day 4 conclusions ‚Üí Day 5 direction)

Here are the best next moves, in order:
	1.	Acknowledge censoring
	‚Ä¢	Our diagnostics are partly distorted by the y=5 cap.
	‚Ä¢	Linear regression isn‚Äôt ideal for censored targets.
	2.	Run a quick heteroskedasticity check (optional but useful)
	‚Ä¢	Breusch‚ÄìPagan test (you already have the code pattern from earlier days).
	‚Ä¢	Even if it flags heteroskedasticity, the plots already suggest it.
	3.	Plan improvements we can try (later days)
	‚Ä¢	Transform target: try log1p(y) (helps variance issues sometimes, though the cap complicates it).
	‚Ä¢	Try a model that handles non-linearities better (Decision Tree / Random Forest / Gradient Boosting).
	‚Ä¢	Consider methods designed for censoring (advanced topic): Tobit / censored regression.
	4.	Day 5 focus
	‚Ä¢	Outliers + leverage + influence (Cook‚Äôs distance)
	‚Ä¢	Identify points with extreme residuals and see if they‚Äôre legitimate or data quirks.

    
Interpolate "What patterns mean (simple mapping)" explanation with the charts and help diganoze the shape  (Predicted vs Residuals) i dont either see a clear curve or fan/funnel shape
mapping, applied to our charts

What we‚Äôre looking at
	‚Ä¢	x-axis: Predicted y
	‚Ä¢	y-axis: Residual (y - y)
	‚Ä¢	A ‚Äúperfect‚Äù world would look like a random cloud centered around 0, with roughly the same vertical spread everywhere.

‚∏ª

1) The most obvious shape: the sharp diagonal ‚Äúedge‚Äù

In our plot there‚Äôs a clean diagonal line (a hard boundary). That is not a curve or a funnel.

‚úÖ Diagnosis: this is caused by the target cap at y = 5 (censoring).

Why:
	‚Ä¢	residual = pred ‚àí actual
	‚Ä¢	actual can‚Äôt exceed 5
	‚Ä¢	so residual can‚Äôt be less than (pred ‚àí 5)
That creates the diagonal boundary.

What this means in our mapping:
This isn‚Äôt ‚Äúnon-linearity‚Äù or ‚Äúheteroskedasticity‚Äù by itself ‚Äî it‚Äôs a data constraint that shapes residuals.

‚∏ª

2) Do we see a curve?

A curve would look like the residual cloud bending:
	‚Ä¢	residuals mostly positive at low predictions and mostly negative at mid predictions, etc.
	‚Ä¢	like a smile or frown pattern around the 0 line

In our plot, the big cloud (predictions roughly 0‚Äì4) is not clearly bending into a smooth curve. It looks more like a blob around zero.

‚úÖ Diagnosis: We do not have strong visual evidence of a classic ‚Äúcurve‚Äù pattern here.

So we can‚Äôt confidently conclude ‚Äúnon-linearity‚Äù just from this plot alone.

(Non-linearity may still exist, but this plot is dominated by the cap boundary.)

‚∏ª

3) Do we see a fan/funnel (heteroskedasticity)?

A funnel would look like:
	‚Ä¢	narrow spread at low predicted values
	‚Ä¢	wider spread as predicted increases (or the reverse)

In our plot, it‚Äôs tricky because:
	‚Ä¢	the cap boundary creates a wedge-like region for high predicted values (>5)
	‚Ä¢	that wedge can look like ‚Äúfanning,‚Äù but it‚Äôs not variance growth ‚Äî it‚Äôs geometry from clipping

So we should focus on the main blob region (pred ~0 to 4):
	‚Ä¢	the vertical spread there looks fairly similar, not a clear ‚Äúopening funnel.‚Äù

‚úÖ Diagnosis: We do not see a clean, textbook funnel shape in the main region.

‚∏ª

4) What shape do we actually see?

We see two regimes:

Regime A: predicted ~0 to 4
	‚Ä¢	looks like a somewhat thick cloud around 0
	‚Ä¢	no strong curve, no strong fan

Regime B: predicted > ~5
	‚Ä¢	points line up near the diagonal boundary
	‚Ä¢	this is because many true values are capped at 5
	‚Ä¢	residuals here are forced by censoring, not just model variance

‚úÖ Our main diagnosis for Predicted vs Residuals:

The plot is heavily influenced by label capping at 5, which creates the sharp diagonal boundary and makes ‚Äúfan/curve‚Äù detection harder. In the uncapped region (pred 0‚Äì4), residuals look reasonably cloud-like without a strong curve or funnel.

That‚Äôs a good, honest diagnosis.
'''


#=========================================================
# Summarization in simple words
#=========================================================
'''
	We fit our Linear Regression model on the training set, predicted on the validation set, and computed residuals from y_val and y_pred. 
    
    We evaluated RMSE and used diagnostic plots‚Äî
    	* Actual vs Predicted, 
        * Residuals vs Predicted (fitted), and 
        * a histogram of residuals‚Äîto look for assumption violations like non-linearity (curved residual pattern) or heteroskedasticity (funnel/fan). 
        
    In our case, the California Housing target is capped at 5.0 (‚âà $500k), 
    which creates an artificial boundary in residual plots (a diagonal floor/ceiling effect), making classic patterns harder to read.
'''