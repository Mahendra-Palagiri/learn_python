'''Week 6 Â· Day 4 â€” Regression Diagnostics (Residuals, Non-Linearity, Heteroskedasticity)

Topics
	â€¢	What residuals are and why diagnostics matter
	â€¢	Residual plots to detect:
	â€¢	non-linearity (curvature)
	â€¢	heteroskedasticity (fan / funnel shape)
	â€¢	outliers (extreme residuals)
	â€¢	â€œGood vs suspiciousâ€ diagnostic patterns
	â€¢	Quick tests (optional): Breuschâ€“Pagan for heteroskedasticity

ğŸ¯ Learning Goals

By the end of Day 4, we will be able to:
	â€¢	Generate residual diagnostics plots correctly
	â€¢	Recognize common failure patterns (curve, fan, clusters)
	â€¢	Explain what each pattern suggests we should try next
	â€¢	Understand why â€œpretty R^2â€ is not enough

'''

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm


from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from statsmodels.stats.diagnostic import het_breuschpagan


# ==========================================================
# Step 1 â€” Fit a model we will diagnose
# ==========================================================
# 1ï¸âƒ£ Load and Split data
chdf = fetch_california_housing(as_frame=True).frame

# print(chdf.info())

target = 'MedHouseVal'
X = chdf.drop(columns=[target])
Y = chdf[target]

X_trainval,X_test,Y_trainval,Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)
X_train,X_val,Y_train,Y_val = train_test_split(X_trainval,Y_trainval,test_size=0.2,random_state=42)

# 2ï¸âƒ£ Fit Model
lr = LinearRegression()
lr.fit(X_train,Y_train)

# 3ï¸âƒ£ Predict based on validation dataset
yval_pred = lr.predict(X_val)
residuals = yval_pred-Y_val
rmse = np.sqrt(mean_squared_error(Y_val,yval_pred))

print("\n==============================================================================")
print("Validation RMSE:",rmse,"\n")

# ==========================================================
# Step 2 â€” Diagnostic plot set
# ==========================================================

# Plot A: Residuals vs Predicted
plt.figure()
plt.scatter(yval_pred,residuals)
plt.axhline(0)
plt.xlabel("Predicted Y (Val)")
plt.ylabel("Residucals (Val)")
plt.title("Predicted vs Residuals -- Validation Set")
# plt.show()

'''
What we want: random cloud centered around 0
Suspicious: curve or fan shape
'''

# Plot B: Residuals vs Actual. (Useful when predicted values are clipped or bunched.)
plt.figure()
plt.scatter(Y_val,residuals)
plt.axhline(0)
plt.xlabel("Actual Y (Val)")
plt.ylabel("Residuals (Val)")
plt.title("Acutal vs Residual  --> Validation Set")
# plt.show()


# Plot C: Predicted vs Actual with diagonal
plt.figure()
plt.scatter(Y_val,yval_pred)
min_y = min(Y_val.min(), yval_pred.min())
max_y = max(Y_val.max(), yval_pred.max())
plt.plot([min_y,max_y],[min_y,max_y])
plt.xlabel("Actual (Val)")
plt.ylabel("Predicted (Val)")
plt.title("Actual vs Predicted --> Validation Set")
# plt.show()

# Plot D: Residual histogram (distribution sanity)
plt.figure()
plt.hist(residuals,bins=50)
plt.xlabel("Residual")
plt.ylabel("Count")
plt.title("Residual Distribution --> Validation Set")
# plt.show()


# ==========================================================
# Step 3 :: Diagnostic plot set (ALL IN ONE FIGURE)
# ==========================================================

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot A: Residuals vs Predicted
ax = axes[0, 0]
ax.scatter(yval_pred, residuals, alpha=0.6)
ax.axhline(0)
ax.set_xlabel("Predicted Y (Val)")
ax.set_ylabel("Residuals (Val)")
ax.set_title("Predicted vs Residuals â€” Validation Set")

# Plot B: Residuals vs Actual
ax = axes[0, 1]
ax.scatter(Y_val, residuals, alpha=0.6)
ax.axhline(0)
ax.set_xlabel("Actual Y (Val)")
ax.set_ylabel("Residuals (Val)")
ax.set_title("Actual vs Residuals â€” Validation Set")

# Plot C: Predicted vs Actual with diagonal
ax = axes[1, 0]
ax.scatter(Y_val, yval_pred, alpha=0.6)
min_y = min(Y_val.min(), yval_pred.min())
max_y = max(Y_val.max(), yval_pred.max())
ax.plot([min_y, max_y], [min_y, max_y])
ax.set_xlabel("Actual (Val)")
ax.set_ylabel("Predicted (Val)")
ax.set_title("Actual vs Predicted â€” Validation Set")

# Plot D: Residual histogram
ax = axes[1, 1]
ax.hist(residuals, bins=50)
ax.set_xlabel("Residual")
ax.set_ylabel("Count")
ax.set_title("Residual Distribution â€” Validation Set")

plt.tight_layout()
fig.savefig("week6/4.1Regression_diagnostics.png", dpi=200, bbox_inches="tight")
# plt.show()

"""
![Diagnostics](week6_day4_regression_diagnostics.png)
"""

# ==========================================================
# Step 4 :: Breuschâ€“Pagan test for heteroskedasticity
# ==========================================================

X_val_sm = sm.add_constant(X_val)
ols = sm.OLS(Y_val,X_val_sm).fit()

bp = het_breuschpagan(ols.resid,X_val_sm)
labels = ["LM Stat", "LM p-value", "F Stat", "F p-value"]

print("\nBreuschâ€“Pagan test:")
for name, val in zip(labels, bp):
    print(name, ":", val)

'''
Breuschâ€“Pagan test:
LM Stat : 300.6642950412045
LM p-value : 2.9735644217256013e-60
F Stat : 41.234070953073186
F p-value : 3.1039460445456923e-63

Interpretation (simple):
	â€¢	low p-value suggests heteroskedasticity is present
'''

''' ---- RETROSPECTION -----
What patterns mean (simple mapping)

**** If we see a â€œcurveâ€ in residuals vs predicted

        Likely:
            â€¢	non-linear relationship we didnâ€™t model
        Next try:
            â€¢	polynomial features
            â€¢	interactions
            â€¢	log transforms

**** If we see a â€œfanâ€ / â€œfunnelâ€

        Likely:
            â€¢	heteroskedasticity (variance changes with prediction level)
        Next try:
            â€¢	transform y (like log)
            â€¢	robust regression / robust SE (later)
            â€¢	different model class

**** If we see extreme points far away

        Likely:
            â€¢	outliers or influential points
        Next try:
            â€¢	examine those rows
            â€¢	Cookâ€™s distance (Day 5)


# EXPLANATION ON THE GRAPH (4.1Regression_diagnostics.png)
1) Predicted vs Residuals (top-left): why that sharp diagonal â€œedgeâ€?

We defined residuals as:

\text{residual} = \hat{y} - y

In the California Housing dataset, the target MedHouseVal is capped at 5.0 (in $100k units). So:

y \le 5

That means residuals must satisfy:

\text{residual} = \hat{y} - y \ge \hat{y} - 5

So thereâ€™s a hard lower boundary line:

\text{residual} = \hat{y} - 5

âœ… That is exactly the sharp diagonal â€œfloorâ€ youâ€™re seeing.
Points canâ€™t fall below it because the true y canâ€™t exceed 5.

Interpretation: This is not â€œa model bug.â€ Itâ€™s a dataset label cap creating a geometric constraint in residual space.

â¸»

2) Actual vs Residuals (top-right): why the vertical band at Actual = 5?

Same reason: many samples have Actual y = 5 (hit the cap).
So we see a vertical wall at x = 5.

Also notice the residuals near actual=5 spread a lot â€” thatâ€™s telling us:
	â€¢	for capped cases, the model can predict above/below 5, but the true label is stuck at 5
	â€¢	residuals there can look systematically biased because the data is censored.

â¸»

3) Actual vs Predicted (bottom-left): why the tall vertical column at Actual = 5?

Again: lots of points have Actual=5, so we get a vertical stack.

Also, notice we sometimes predict above 5 (up to ~7+). Thatâ€™s normal for plain linear regression because it doesnâ€™t â€œknowâ€ the label cap. It will happily extrapolate beyond 5.

â¸»

4) Residual distribution (bottom-right): what do we see?

The histogram is:
	â€¢	roughly centered near 0 (good)
	â€¢	but not perfectly symmetric, and thereâ€™s a heavier tail (common in real-world regression)

With this dataset, we should expect residual weirdness because:
	â€¢	the target is censored at 5 (so errors canâ€™t behave like a clean bell curve),
	â€¢	and the true relationship is not perfectly linear.

â¸»

What does this mean for Day 4 diagnostics?

What looks â€œgoodâ€
	â€¢	We do see a reasonable diagonal trend in Actual vs Predicted.
	â€¢	Residuals are broadly around 0 for a big chunk of the data.

What looks â€œsuspicious / informativeâ€
	â€¢	The cap-driven boundary (diagonal floor + vertical wall) is telling us our target is censored.
	â€¢	The spread of residuals varies with prediction/actual in places â†’ likely heteroskedasticity (variance not constant).
	â€¢	There may also be some non-linearity (common with housing).

â¸»

What we should do next (our Day 4 conclusions â†’ Day 5 direction)

Here are the best next moves, in order:
	1.	Acknowledge censoring
	â€¢	Our diagnostics are partly distorted by the y=5 cap.
	â€¢	Linear regression isnâ€™t ideal for censored targets.
	2.	Run a quick heteroskedasticity check (optional but useful)
	â€¢	Breuschâ€“Pagan test (you already have the code pattern from earlier days).
	â€¢	Even if it flags heteroskedasticity, the plots already suggest it.
	3.	Plan improvements we can try (later days)
	â€¢	Transform target: try log1p(y) (helps variance issues sometimes, though the cap complicates it).
	â€¢	Try a model that handles non-linearities better (Decision Tree / Random Forest / Gradient Boosting).
	â€¢	Consider methods designed for censoring (advanced topic): Tobit / censored regression.
	4.	Day 5 focus
	â€¢	Outliers + leverage + influence (Cookâ€™s distance)
	â€¢	Identify points with extreme residuals and see if theyâ€™re legitimate or data quirks.

    
Interpolate "What patterns mean (simple mapping)" explanation with the charts and help diganoze the shape  (Predicted vs Residuals) i dont either see a clear curve or fan/funnel shape
mapping, applied to our charts

What weâ€™re looking at
	â€¢	x-axis: Predicted \hat{y}
	â€¢	y-axis: Residual (\hat{y} - y)
	â€¢	A â€œperfectâ€ world would look like a random cloud centered around 0, with roughly the same vertical spread everywhere.

â¸»

1) The most obvious shape: the sharp diagonal â€œedgeâ€

In our plot thereâ€™s a clean diagonal line (a hard boundary). That is not a curve or a funnel.

âœ… Diagnosis: this is caused by the target cap at y = 5 (censoring).

Why:
	â€¢	residual = pred âˆ’ actual
	â€¢	actual canâ€™t exceed 5
	â€¢	so residual canâ€™t be less than (pred âˆ’ 5)
That creates the diagonal boundary.

What this means in our mapping:
This isnâ€™t â€œnon-linearityâ€ or â€œheteroskedasticityâ€ by itself â€” itâ€™s a data constraint that shapes residuals.

â¸»

2) Do we see a curve?

A curve would look like the residual cloud bending:
	â€¢	residuals mostly positive at low predictions and mostly negative at mid predictions, etc.
	â€¢	like a smile or frown pattern around the 0 line

In our plot, the big cloud (predictions roughly 0â€“4) is not clearly bending into a smooth curve. It looks more like a blob around zero.

âœ… Diagnosis: We do not have strong visual evidence of a classic â€œcurveâ€ pattern here.

So we canâ€™t confidently conclude â€œnon-linearityâ€ just from this plot alone.

(Non-linearity may still exist, but this plot is dominated by the cap boundary.)

â¸»

3) Do we see a fan/funnel (heteroskedasticity)?

A funnel would look like:
	â€¢	narrow spread at low predicted values
	â€¢	wider spread as predicted increases (or the reverse)

In our plot, itâ€™s tricky because:
	â€¢	the cap boundary creates a wedge-like region for high predicted values (>5)
	â€¢	that wedge can look like â€œfanning,â€ but itâ€™s not variance growth â€” itâ€™s geometry from clipping

So we should focus on the main blob region (pred ~0 to 4):
	â€¢	the vertical spread there looks fairly similar, not a clear â€œopening funnel.â€

âœ… Diagnosis: We do not see a clean, textbook funnel shape in the main region.

â¸»

4) What shape do we actually see?

We see two regimes:

Regime A: predicted ~0 to 4
	â€¢	looks like a somewhat thick cloud around 0
	â€¢	no strong curve, no strong fan

Regime B: predicted > ~5
	â€¢	points line up near the diagonal boundary
	â€¢	this is because many true values are capped at 5
	â€¢	residuals here are forced by censoring, not just model variance

âœ… Our main diagnosis for Predicted vs Residuals:

The plot is heavily influenced by label capping at 5, which creates the sharp diagonal boundary and makes â€œfan/curveâ€ detection harder. In the uncapped region (pred 0â€“4), residuals look reasonably cloud-like without a strong curve or funnel.

Thatâ€™s a good, honest diagnosis.
'''