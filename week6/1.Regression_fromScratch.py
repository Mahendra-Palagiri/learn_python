''' Week 6 Â· Day 1 â€” Regression From Scratch (No Libraries)

ğŸ¯ Learning Goal

By the end of today, we will understand:
    â€¢ What linear regression is actually doing (predicting with a line: Å· = wx + b)
    â€¢ What â€œlossâ€ means in regression and why MSE is commonly used
    â€¢ How gradient descent updates w and b (what dw and db represent intuitively)
    â€¢ How learning rate affects training behavior (smooth â†’ oscillation â†’ explosion)
    â€¢ How to verify training worked using evidence:
        - Loss curve decreases over iterations
        - Learned line visually fits the data
        - Learned (w, b) approach the true (w_true, b_true) on synthetic data
    â€¢ Why feature scaling impacts optimization (what happens if X is multiplied by 100)

This is where we stop treating regression like a black box and start understanding the engine.

'''