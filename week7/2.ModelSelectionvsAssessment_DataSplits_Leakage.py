''' Week 7 ‚Äî Day 2: Model Selection vs Model Assessment + Data Splits & Leakage

üìö Topics
	‚Ä¢	Model selection vs model assessment (tune vs report)
	‚Ä¢	Correct splitting patterns:
        ‚Ä¢	Train/Validation/Test
        ‚Ä¢	Cross-validation for tuning + final holdout test
	‚Ä¢	Data leakage (what it is, why it breaks truth)
	‚Ä¢	Common leakage patterns (the usual ‚Äúgotchas‚Äù)
	‚Ä¢	The ‚Äúgolden rule‚Äù: preprocessing/feature decisions must happen inside the training-only process

üéØ Learning Goals
	‚Ä¢	Clearly state the difference between selection and assessment
	‚Ä¢	Explain why tuning on the test set is invalid (even ‚Äújust once‚Äù)
	‚Ä¢	Identify at least 4 common leakage sources
	‚Ä¢	Describe a clean evaluation workflow we‚Äôll use going forward

'''

# ==========================================================
# 1 ‚Äî Model selection vs model assessment (the big separation)
# ==========================================================
'''
	‚Ä¢	Model selection = deciding which model + which hyperparameters to use.
            Output: ‚ÄúThis is the chosen pipeline/config.‚Äù
	‚Ä¢	Model assessment = estimating how well the final chosen approach performs on unseen data.
            Output: ‚ÄúThis is the performance we can honestly report.‚Äù

    Why this matters: if we use the same data to choose and judge, 
    we accidentally reward luck and overfitting to the evaluation itself.

'''

# ==========================================================
# 2 ‚Äî Splitting patterns we‚Äôll follow
# ==========================================================

'''
    Pattern A ‚Äî Classic (simple projects)
        1.	Split once into train and test
        2.	Inside train: split again into train and validation (or use CV)
        3.	Use validation/CV for selection
        4.	Touch test only once at the end for assessment

    Pattern B ‚Äî Best practice (most ML workflows)
        1.	Split into train and test
        2.	Use cross-validation on train for selection + tuning
        3.	Refit best model on full train
        4.	Evaluate once on test

    The key idea: test set is the ‚Äúfinal exam,‚Äù not the practice quiz.
'''


# ==========================================================
# 3 ‚Äî Data leakage (the silent performance inflator)
# ==========================================================
'''
Leakage =   using information during training (directly or indirectly) 
            that would not be available at prediction time, 
            or letting validation/test influence training decisions.

Result: performance looks amazing‚Ä¶ then collapses in real life
'''

# ==========================================================
# 4 ‚Äî Common leakage examples (must recognize)
# ==========================================================
'''
	‚Ä¢	Scaling/normalizing before splitting (validation stats leak into training)
	‚Ä¢	Encoding categories before splitting (validation categories/statistics leak)
	‚Ä¢	Imputation using whole dataset (mean/median computed using validation/test)
	‚Ä¢	Feature selection using full data (correlation, mutual info, p-values done on all data)
	‚Ä¢	Target leakage features:
	‚Ä¢	‚Äúfuture‚Äù information (post-outcome fields)
	‚Ä¢	aggregates that include the target period
	‚Ä¢	Time series shuffle leakage (training sees future patterns)

    Core fix (simple rule):

    Anything that learns from data must be fit using training-only, ideally via a Pipeline.
'''

# ==========================================================
# 5 ‚Äî The clean workflow (what we‚Äôll use going forward)
# ==========================================================
'''
	1.	Decide metric + baseline
	2.	Split train/test
	3.	Build Pipeline (prep + model)
	4.	Tune using CV on train
	5.	Pick best based on CV mean + stability
	6.	Final one-time score on test

'''


#=========================================================
# Summarization in simple words
#=========================================================
'''
    Today we understood model selection vs model assessment. 
    
    Model selection is choosing the model and hyperparameters using validation data or cross-validation on the training set. 
    Model assessment is evaluating the final chosen model on the held-out test set, ideally only once. 
    
    This separation prevents data leakage / overfitting to the test set and gives a more honest estimate of real-world performance

    We also learned common causes of data leakage: 
        doing encoding, imputation, or scaling before splitting, and performing feature selection using the full dataset. 
    
    If test data influences any preprocessing, feature decisions, or tuning, 
    the evaluation becomes contaminated‚Äîperformance looks better than it truly is, 
    and the model can overfit to the evaluation, 
    which often leads to weaker performance in real-world unseen data
'''