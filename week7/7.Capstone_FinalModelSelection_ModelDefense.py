'''Week 7 â€” Day 7: Capstone â€” Final Model Selection + Model Defense

ðŸ“š Topics
	â€¢	Final selection workflow (selection vs assessment discipline)
	â€¢	Evidence-based comparison (CV mean + std + simplicity)
	â€¢	â€œModel Defenseâ€ write-up (why this model, what risks remain)
	â€¢	Final checklist: leakage, metric choice, stability, next improvements

ðŸŽ¯ Learning Goals
	â€¢	Choose a final model using evidence (not vibes)
	â€¢	Identify next steps to improve performance (features/model family/thresholding)
	â€¢	Lock a repeatable template for future projects

'''


#=========================================================
# Week 7. --> RetroSpection
#=========================================================
'''
1) Decision summary (based on Day 4â€“6 evidence)

Candidate models tested (5-fold Stratified CV, F1):
	â€¢	LogReg: mean 0.6495, std 0.0132
	â€¢	DecisionTree: mean 0.5536, std 0.0069
	â€¢	RandomForest: mean 0.5913, std 0.0070

Final choice: LogisticRegression Pipeline
Reason: Highest mean F1 by a meaningful margin. Stability is acceptable. Also simplest to deploy and explain.

â¸»

2) Final model configuration (best from Day 5)

Best params found:
	â€¢	C = 0.01, penalty = l1, class_weight = None, scaler = StandardScaler

So the final pipeline is:
	â€¢	Numeric: median impute â†’ StandardScaler
	â€¢	Categorical: most_frequent impute â†’ OneHotEncoder
	â€¢	Model: LogisticRegression (liblinear, L1, C=0.01)

â¸»

3) Model Defense 
We selected a leakage-safe Pipeline (imputation + scaling/encoding + classifier) 
and used 5-fold Stratified cross-validation with F1 as the metric. 

Logistic Regression achieved the best average F1 (~0.65) 
compared to Decision Tree (~0.55) and Random Forest (~0.59). 

The CV standard deviation (~0.013) indicates reasonably stable performance across folds. 
Hyperparameter tuning (C, penalty, class_weight, scaler choice) did not materially improve results, 
suggesting the current performance is limited more by feature signal/noise than by tuning. 

The final model is simple, interpretable, and consistent, making it the best choice for this dataset 
under the current feature set.

â¸»

4) Risks and what to monitor
	â€¢	Noise ceiling / limited signal: tuning didnâ€™t help much â†’ likely feature-limited
	â€¢	Synthetic dataset limitation: CV performance might not transfer to real Titanic distribution
	â€¢	Threshold sensitivity: F1 depends on decision threshold (0.5 may not be optimal)
	â€¢	Data drift: if class proportions change, F1 behavior changes

â¸»

5) Next improvements (practical)
	â€¢	Add stronger predictive features (if available): family size, title extraction, ticket group features (for real Titanic)
	â€¢	Try threshold tuning for F1 (choose threshold that maximizes CV F1)
	â€¢	Try gradient boosting models (XGBoost/LightGBM-style equivalents in sklearn: HistGradientBoosting) using same pipeline discipline
	â€¢	Error slicing: evaluate F1 by subgroup (Sex, Cabin missing vs present, Embarked)
'''