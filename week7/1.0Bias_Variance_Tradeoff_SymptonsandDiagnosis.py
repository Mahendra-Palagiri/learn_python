''' Week 7 ‚Äî Day 1: Bias‚ÄìVariance Tradeoff (Symptoms + Diagnosis)

Topics
	‚Ä¢	Bias vs Variance (practical definitions)
	‚Ä¢	Underfitting vs Overfitting (train/validation symptoms)
	‚Ä¢	How model complexity shifts bias/variance:
	    ‚Ä¢	linear vs polynomial degree
	    ‚Ä¢	regularization strength (Ridge/Lasso)
	    ‚Ä¢	tree depth / min samples
	‚Ä¢	Learning curves (train vs validation error vs training set size):
	    ‚Ä¢	data scarcity vs model mismatch vs noise ceiling

üéØ Learning Goals
	‚Ä¢	Explain bias and variance in simple words
	‚Ä¢	Diagnose underfit vs overfit using train vs validation results
	‚Ä¢	Pick the correct ‚Äúcomplexity knob‚Äù to turn (increase vs decrease complexity)
	‚Ä¢	Use learning-curve patterns to decide what‚Äôs limiting performance (data, model, or noise)
	
'''


# ==========================================================
# 1 ‚Äî Bias vs Variance
# ==========================================================
'''
Bias
	‚Ä¢	Error from a model being too simple/too restricted to capture the real pattern
    ‚Ä¢	How to recognize bias. --> "The model always missess the pattern in a similar way"

Variance
    ‚Ä¢	Error from a model being too sensitive to the specific training data
    ‚Ä¢	How to recognize Variance. --> "Learning noise and quirks not the real signal"

The variance of these factors
    ‚Ä¢	High Bias --> Underfit
    ‚Ä¢	High Variance --> Overfit
'''


# ==========================================================
# 2 ‚Äî Symptoms (Train vs Validation)
# ==========================================================
'''
Train Performance       Validation Performance              What it means?
-----------------       ----------------------              ---------------
bad                     bad                                 Underfitting (high bias)
good                    bad                                 Overfitting (high variance)
good                    good (slightly worse than train)    Healthy/generalizing 


Important: 
    ‚Ä¢	validation being a bit worse than train is normal. The problem is when the gap is large

Key instinct:
	‚Ä¢	Underfitting ‚Üí model needs more capacity (or better features)
	‚Ä¢	Overfitting ‚Üí model needs more constraint (regularization / simpler model) or more data
'''


# ==========================================================
# 3 ‚Äî The "Complexity Knobs" (how we shift the tradeoff)
# ==========================================================
'''
A) Linear vs Polynomial degree
	‚Ä¢	Linear model (low complexity): tends to have higher bias, lower variance.
	‚Ä¢	Polynomial features (higher degree): increases complexity ‚Üí can reduce bias but can increase variance (overfit risk).

    Rule:
    ‚Ä¢	If underfitting ‚Üí try adding polynomial degree (carefully)
    ‚Ä¢	If overfitting ‚Üí reduce degree or regularize

B) Regularization strength (Ridge/Lasso)

    Regularization = adding a penalty so the model doesn‚Äôt become too wild.
	‚Ä¢	Stronger regularization (higher alpha/lambda):
        ‚Ä¢	reduces variance (less overfitting)
        ‚Ä¢	may increase bias (can underfit if too strong)
	‚Ä¢	Weaker regularization:
        ‚Ä¢	reduces bias (fits more)
        ‚Ä¢	may increase variance (overfit risk)

    Ridge vs Lasso (one-liner):
	‚Ä¢	Ridge shrinks all coefficients
	‚Ä¢	Lasso can shrink some to zero (feature selection effect)

C) Tree depth / min samples
    Trees can become extremely flexible.
	‚Ä¢	Deeper tree (higher max_depth, smaller min_samples_leaf):
	    ‚Ä¢	lower bias, higher variance (overfit risk)
	‚Ä¢	Shallower tree (lower max_depth, larger min_samples_leaf):
	    ‚Ä¢	higher bias, lower variance (more stable)
'''

# ==========================================================
# 4 ‚Äî Learning Curves. (Best diagnositc tool to understand these factors)
# ==========================================================
'''
Learning curves plot train error and validation error as training data increases.

We use them to answer: Is the limit coming from data, model simplicity, or unavoidable noise?

Pattern 1 ‚Äî Model mismatch (high bias / underfitting)
	‚Ä¢	Train error: high
	‚Ä¢	Validation error: high
	‚Ä¢	Curves close together, both bad

> Meaning: model too simple / features not expressive 
> Typical fix: increase complexity (better features, polynomial/interactions, different model)

Pattern 2 ‚Äî Overfitting (high variance)
	‚Ä¢	Train error: low
	‚Ä¢	Validation error: higher
	‚Ä¢	Noticeable gap

> Meaning: model too flexible for the data
> Typical fix: regularization, simpler model, constrain tree, or add more data

Pattern 3 ‚Äî Data scarcity
	‚Ä¢	Validation improves as more data is added (gap may shrink)

> Meaning: more data likely helps

Pattern 4 ‚Äî Noise ceiling
	‚Ä¢	Both curves flatten and stop improving past a point

> Meaning: data has noise/limits; improvements require better features, cleaner data, or reframing the problem

'''

#=========================================================
# Summarization in simple words
#=========================================================
'''
    Today we learned what bias and variance mean and how to recognize them from train vs validation behavior. 
    
    If both training and validation performance are bad, that indicates high bias (underfitting). 
    If training performance is good but validation performance is bad, that indicates high variance (overfitting). 
    
    We also learned the ‚Äúknobs‚Äù to adjust complexity: 
        To reduce high bias we generally increase complexity gradually 
            (e.g., polynomial features, deeper trees, or weaker regularization). 
        To reduce high variance we generally reduce complexity 
            (e.g., simpler/linear models, shallower trees, or stronger regularization). 
    
    Finally, learning curves (train vs validation error as data increases) help decide whether the limitation is 
        data scarcity, 
        model mismatch/high bias, or 
        a noise ceiling (irreducible error).
'''