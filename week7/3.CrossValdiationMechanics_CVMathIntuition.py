'''Week 7 ‚Äî Day 3: Cross-Validation Mechanics + CV Math Intuition 

üìö Topics
	‚Ä¢	What cross-validation (CV) is doing mechanically (k-fold step-by-step)
	‚Ä¢	Choosing k (small vs large) and the tradeoffs
	‚Ä¢	Stratified k-fold (why classification needs it when classes are imbalanced)
	‚Ä¢	Interpreting CV results as a distribution (mean + variance)
	‚Ä¢	CV ‚Äúmath idea‚Äù (why averaging folds estimates generalization, and what it‚Äôs not)

üéØ Learning Goals
	‚Ä¢	Explain k-fold CV in 4‚Äì5 steps without hand-waving
	‚Ä¢	State what changes when k is small vs large
	‚Ä¢	Explain why stratification matters (in one clean sentence)
	‚Ä¢	Interpret ‚ÄúCV mean‚Äù vs ‚ÄúCV std‚Äù (stability)
	‚Ä¢	Explain the key limitation: CV is an estimate, not truth

'''

# ==========================================================
# 1 ‚Äî What k-fold CV actually does (mechanics)
# ==========================================================
'''
    Given a dataset, choose k (example: 5).
	1.	Split the data into k equal-ish folds
	2.	For fold i:
        ‚Ä¢	Train on k‚àí1 folds
        ‚Ä¢	Validate on the remaining fold
	3.	Record the score for that fold
	4.	Repeat until each fold has been the validation fold once
	5.	Aggregate the scores:
        ‚Ä¢	mean = typical performance estimate
        ‚Ä¢	std = stability (how much performance swings by fold)

    Key property: every row is used for training (most of the time) and for validation exactly once.
'''

# ==========================================================
# 2 ‚Äî What happens when k is small vs large
# ==========================================================
'''
    ‚Ä¢	Small k (e.g., 3‚Äì5):
        ‚Ä¢	Faster
        ‚Ä¢	Slightly higher bias in estimate (less training data per fold)
        ‚Ä¢	Usually good default
	‚Ä¢	Large k (e.g., 10):
        ‚Ä¢	More compute
        ‚Ä¢	Lower bias in estimate (training uses more data each fold)
        ‚Ä¢	Can have higher variance depending on dataset
	‚Ä¢	Extreme: Leave-One-Out (k = n):
        ‚Ä¢	Very expensive
        ‚Ä¢	Often high variance in practice

    Rule of thumb we‚Äôll use:
        ‚Ä¢	Start with 5-fold (or 10-fold when dataset is small and compute is OK)
'''

# ==========================================================
# 3 ‚Äî Stratified k-fold (classification)
# ==========================================================
'''
    In classification, if classes are imbalanced, random folds can accidentally create a fold with:
	‚Ä¢	too few positives (or none)
	‚Ä¢	weird class ratios

    Stratified k-fold keeps class proportions roughly consistent in each fold.

    One-liner:
        ‚Ä¢	Stratification prevents ‚Äúlucky/unlucky folds‚Äù caused by imbalance.

    Pratctial example from our previous learnings :
    ----------------------------------------------
    Class proportion just means: what fraction of the rows belong to each class (each label) in the target.

    Example from our earlier-style classification work (like Titanic ‚ÄúSurvived‚Äù):
        ‚Ä¢	Target Survived has two classes: 0 = did not survive, 1 = survived
        ‚Ä¢	Suppose the full dataset looks like this:
            ‚Ä¢	62% are Survived = 0
            ‚Ä¢	38% are Survived = 1

    Those percentages are the class proportions.

    Now if we do 5-fold CV:
        ‚Ä¢	With StratifiedKFold, each fold will be kept close to that same split, roughly:
        ‚Ä¢	Fold 1: ~62% zeros, ~38% ones
        ‚Ä¢	Fold 2: ~62% zeros, ~38% ones
        ‚Ä¢	‚Ä¶and so on

    Without stratification, one fold might accidentally become skewed, like:
        ‚Ä¢	Fold 3: 80% zeros, 20% ones (or worse, almost no ‚Äú1‚Äùs)

    That causes noisy/unstable validation scores‚Äîbecause the validation fold no longer represents the real class mix we‚Äôre trying to generalize to.
'''

# ==========================================================
# 4 ‚Äî CV score is a distribution (not one number)
# ==========================================================
'''
    Instead of ‚Äúthe score,‚Äù CV gives multiple scores:
	‚Ä¢	Mean score: average expected performance
	‚Ä¢	Std (spread): how stable the model is

    Interpretation:
	‚Ä¢	High mean + low std ‚Üí good and reliable
	‚Ä¢	High mean + high std ‚Üí risky (depends on split)
	‚Ä¢	Lower mean + low std ‚Üí stable but maybe limited (bias)
'''

# ==========================================================
# 5 ‚Äî CV math intuition (lightweight but real)
# ==========================================================
'''
    CV is trying to estimate generalization error: performance on unseen data from the same distribution.

    Why averaging folds helps:
        ‚Ä¢	Each fold score is like one ‚Äúmini test‚Äù on held-out data
        ‚Ä¢	Averaging reduces dependence on a single lucky split

    What CV is not:
        ‚Ä¢	Not a guarantee of real-world performance
        ‚Ä¢	Not safe if there‚Äôs leakage (Day 2 rules still apply)
        ‚Ä¢	Not valid if the data isn‚Äôt i.i.d. (independent & identically distributed), e.g. time series without special splitting
'''

#=========================================================
# Summarization in simple words
#=========================================================
'''
    Today we learned how cross-validation works. 
    
    The data is split into K folds, and for each iteration the model is trained on K‚àí1 folds 
    and validated on the remaining fold. 
    
    After running all folds, we compute mean and std of the scores. 
    The mean is a more reliable indicator than a single split because it reduces the impact of lucky/unlucky folds, 
    and the std shows how stable the model is across splits. 
    
    We also discussed the tradeoff of choosing K: 
        smaller K is faster but uses less training data per fold, 
        while larger K typically costs more compute and may produce a noisier estimate in some settings (especially very large K like LOOCV). 
        
    We understood Stratified K-fold, 
    where each fold keeps the class distribution close to the original dataset, 
    which is important for imbalanced classification. 
    
    Finally, CV is an estimate‚Äînot a guarantee‚Äîso it can be misleading if 
    there is data leakage or 
    if the data is not IID (Independent and Identically Distributed).

'''