''' *** Week 5 Â· Day 4 â€” ROC Curve, AUC & Decision Thresholds
ğŸ¯ Learning Goal

Today we will understand:
	â€¢	Why classification â‰  fixed threshold. (Predict vs Predict_Proba)
	â€¢	How to evaluate a classifier independent of thresholds
	â€¢	What ROC and AUC really measure
	â€¢	How changing thresholds reshapes precision & recall
	â€¢	Why Day 3 metrics are just one operating point

    This is the conceptual bridge from â€œmetricsâ€ â†’ â€œmodel behaviorâ€.
'''

import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler,OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve,classification_report, roc_auc_score
import matplotlib.pyplot as plt

titdf = pd.read_csv('./data/week5/titanic_synthetic.csv')
# print(titdf.info())
# print(titdf.describe(include='all'))

num_cols = ['Age','Fare']
cat_cols = ['Embarked','Sex']
features = num_cols + cat_cols
target = 'Survived'

num_pipe = Pipeline([
    ('impute',SimpleImputer(strategy='median')),
    ('scaler',RobustScaler())
])

cat_pipe = Pipeline([
    ('impute',SimpleImputer(strategy='most_frequent')),
    ('encoder',OneHotEncoder(handle_unknown='ignore'))
])

preprocess = ColumnTransformer([
    ('nums',num_pipe,num_cols),
    ('catg',cat_pipe,cat_cols)
])

final_pipe = Pipeline([
    ('preprcsr',preprocess),
    ('model',LogisticRegression(max_iter=1000))
])

X = titdf[features]
Y = titdf[target]

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)

final_pipe.fit(X_train,Y_train)
y_proba = final_pipe.predict_proba(X_test)[:,1]
# print('\n\nY_PROBA --> ',y_proba.min(), y_proba.max(),'\n\n')

finalYpred = final_pipe.predict(X_test) #Under the hood Predit uses (y_proba >= 0.5).astype(int) i.e. its considering only one threshold of 0.5 
acc = accuracy_score(Y_test,finalYpred)
cm = confusion_matrix(Y_test,finalYpred)
# print(cm)

clrpt = classification_report(Y_test,finalYpred)
# print(clrpt)



''' ROC Curve â€” What It Actually Represents
Definitions
	â€¢	TPR (Recall)
        TPR = TP/(TP + FN)
	â€¢	FPR
        FPR = FP/(FP + TN)

    Each threshold gives:
        â€¢	one TPR
        â€¢	one FPR
        â€¢	one point on the ROC curve
'''
fpr, tpr, thresholds = roc_curve(Y_test,y_proba)
# print(thresholds[0], '\n\n Last threshold. --> ', thresholds[-1]) #First and last element values

plt.plot(fpr,tpr, label='Logistic Regression')
plt.plot([0,1],[0,1],'k--',label='Random')
plt.xlabel('False Postivie Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
# plt.show()

rocauc = roc_auc_score(Y_test,y_proba)
print(rocauc) # Output value 0.661006906808626

for t in [0.7,0.6,0.66,0.5,0.4,0.3]:
    y_pred_t = (y_proba >= t).astype(int)
    print(f"\nThreshold = {t}")
    print(classification_report(Y_test, y_pred_t))

#Observation
	# â€¢	Lower threshold â†’ higher recall for class 1
	# â€¢	Lower threshold â†’ lower precision for class 1
	# â€¢	F1 peaks somewhere in the middle


''' --- RETROPSECTION ---
***** 
is roc_auc_score the min or max threshold that can be used i.e. in our example our roc_auc_score is 0.66 is that why when we were generating classification reports with diff thresholds we started with 0.6?
This is a very important clarification, because confusing AUC with thresholds is a common but serious misunderstanding.

Short answer first, then the deep explanation:

NO â€” roc_auc_score has nothing to do with thresholds.
0.66 is NOT a threshold.
It does NOT constrain which thresholds you can use.

Now letâ€™s unpack this properly.

â¸»

1ï¸âƒ£ What AUC actually is

roc_auc_score returns Area Under the ROC Curve.

That number answers one question only:

If I randomly pick one positive and one negative example,
what is the probability that the model ranks the positive higher than the negative?

So:
	â€¢	AUC = 0.66 means
â†’ In 66% of random (positive, negative) pairs,
â†’ the model assigns a higher probability to the positive.

Thatâ€™s it.

It is:
	â€¢	âŒ NOT a probability threshold
	â€¢	âŒ NOT a cutoff
	â€¢	âŒ NOT a min or max allowed value


*****
explain with a detailed example when precision matters , when recall matters if F1 is a mean average why does it matter at all. 
(with diff thresholds the binary classes of survived or not survied is changing on these metrics)

The Core Problem (before metrics)

You have a model that outputs:

Probability that a passenger survived

But probabilities are not decisions.

To make a decision, you must choose a threshold.

Every threshold creates a different world:
	â€¢	who is predicted â€œsurvivedâ€
	â€¢	who is predicted â€œnot survivedâ€
	â€¢	how many mistakes of each type you make

Metrics exist to describe which mistakes you care about.

â¸»

<> Step 1: The Two Types of Mistakes (this is everything)

In binary classification there are only two possible errors:
Error                   Meaning
False Positive (FP)     You predicted Survived, but the passenger did not survive
False Negative (FN)     You predicted Did not survive, but the passenger did survive

Every metric is just a different way of saying:

Which mistake hurts more?

<> Step 2: Precision â€” â€œDonâ€™t Cry Wolfâ€ Metric

Definition (in plain English)

Precision answers:
â€œWhen the model says Survived, how often is it correct?â€

Formula

Precision = TP/(TP + FP)

â¸»

When Precision Matters (detailed examples)

ğŸš¨ Example 1: Medical Alert System (False Positives are expensive)

Imagine:
	â€¢	Model flags patients as having cancer
	â€¢	Each positive prediction triggers:
	â€¢	biopsies
	â€¢	stress
	â€¢	cost
	â€¢	potential harm

If precision is low:
	â€¢	Many healthy patients are told they might have cancer
	â€¢	Massive unnecessary harm

â¡ï¸ You want:
	â€¢	High precision
	â€¢	You are OK missing some real cases (FN)
	â€¢	You want to be very sure when you raise an alarm

How threshold behaves here
	â€¢	Use HIGH threshold (e.g., 0.8)
	â€¢	Only very confident cases are predicted positive
	â€¢	Fewer FP â†’ higher precision
	â€¢	More FN â†’ lower recall

â¸»

<> Step 3: Recall â€” â€œDonâ€™t Miss Anyoneâ€ Metric

Definition (in plain English)

Recall answers:
â€œOf all actual survivors, how many did the model catch?â€

Formula

Recall = TP/(TP + FN)

â¸»

When Recall Matters (detailed examples)

ğŸ§¯ Example 2: Fire Detection System (False Negatives are catastrophic)

Imagine:
	â€¢	Model detects fire in a building
	â€¢	Missing a fire = people die

If recall is low:
	â€¢	Real fires go undetected
	â€¢	Catastrophic outcome

â¡ï¸ You want:
	â€¢	High recall
	â€¢	You accept false alarms (FP)
	â€¢	Better to evacuate unnecessarily than miss a fire

How threshold behaves here
	â€¢	Use LOW threshold (e.g., 0.2)
	â€¢	Many positives detected
	â€¢	FN â†“ â†’ recall â†‘
	â€¢	FP â†‘ â†’ precision â†“

â¸»

<> Step 4: Why Precision and Recall Fight Each Other

This is not a bug â€” it is fundamental.
Lower threshold     Higher threshold
--------------      ----------------
More positives      Fewer positives
FP â†‘                FP â†“
FN â†“                FN â†‘
Recall â†‘            Precision â†‘

You cannot maximize both at the same time unless the problem is perfectly separable (almost never happens).
â¸»

<> Step 5: Where F1 Comes In (and why it matters)

What F1 actually is

F1 = 2 x (Precision x Recall)/(Precision + Recall)


This is not a simple average.

It is a harmonic mean, which:
	â€¢	punishes extreme imbalance
	â€¢	drops sharply if either precision or recall is low

â¸»

Why not just accuracy?

Because accuracy hides imbalance.

Example:
	â€¢	95% of passengers died
	â€¢	Model predicts â€œdiedâ€ for everyone
	â€¢	Accuracy = 95%
	â€¢	Precision/Recall for survivors = 0

Accuracy lies.

â¸»

Why F1 matters (deep reason)

F1 answers:

â€œCan this model make useful positive predictions?â€

	â€¢	If precision = 1.0 and recall = 0.0 â†’ F1 = 0
	â€¢	If recall = 1.0 and precision = 0.0 â†’ F1 = 0
	â€¢	Only when both are reasonably good does F1 rise

So F1 filters out:
	â€¢	â€œI never predict positivesâ€
	â€¢	â€œI predict everything positiveâ€

Both are useless.

â¸»

Step 6: Bring it back to Titanic example

Our model outputs probabilities between:

0.28 â†’ 0.78

Threshold = 0.6
	â€¢	Few predicted survivors
	â€¢	Precision â†‘
	â€¢	Recall â†“

Threshold = 0.3
	â€¢	Many predicted survivors
	â€¢	Recall â†‘
	â€¢	Precision â†“

Threshold â‰ˆ 0.45
	â€¢	Balanced behavior
	â€¢	Precision â‰ˆ Recall
	â€¢	F1 peaks

Thatâ€™s why threshold tuning directly changes these metrics.

â¸»

Step 7: Which metric should YOU care about?

It depends entirely on problem cost:
Problem                         Metric Priority
---------                       ----------------
Medical diagnosis               Recall (donâ€™t miss cases)
Fraud detection                 Precision (donâ€™t block legit users)
Spam filtering                  Precision
Safety alarms                   Recall
Balanced business decision      F1
Ranking users                   AUC

There is no universally â€œbestâ€ metric.

â¸»

Final Mental Model
Precision = â€œWhen I speak, am I right?â€
Recall = â€œDid I miss anyone?â€
F1 = â€œAm I useful at all?â€
Threshold = â€œHow bold do I want to be?â€

Metrics donâ€™t judge the model.
They describe the consequences of your threshold choice.

'''