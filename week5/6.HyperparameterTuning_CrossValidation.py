''' Week 5 ¬∑ Day 6 ‚Äî Hyperparameter Tuning & Cross-Validation

üéØ Learning Goal

By the end of today, we will understand:
	‚Ä¢	Why train/test accuracy is not enough
	‚Ä¢	What cross-validation really measures
	‚Ä¢	Why GridSearchCV is not just ‚Äútrying combinations‚Äù
	‚Ä¢	How to tune C, penalty, solver properly
	‚Ä¢	How to avoid accidentally overfitting the test set

This is where our work starts to resemble real ML practice.

'''

import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler,OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import classification_report


titdf = pd.read_csv('./data/week5/titanic_synthetic.csv')

num_cols = ['Age','Fare']
cat_cols = ['Embarked','Sex']
features = num_cols+cat_cols
target = 'Survived'

num_pipe = Pipeline([
    ('impute',SimpleImputer(strategy='median')),
    ('scaler',RobustScaler())
])

cat_pipe = Pipeline([
    ('impute',SimpleImputer(strategy='most_frequent')),
    ('encoder',OneHotEncoder(handle_unknown='ignore'))
])

pre_prcsr = ColumnTransformer([
    ('nums',num_pipe,num_cols),
    ('catg',cat_pipe,cat_cols)
])

final_pipe = Pipeline([
    ('preprcsr',pre_prcsr),
    ('model',LogisticRegression(max_iter=1000))
])

X = titdf[features]
Y = titdf[target]

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)

param_grid = {
    'model__C':[0.01,0.1,1,10],
    'model__penalty':['l1','l2'],
    'model__solver': ['liblinear']
}

grid = GridSearchCV(
    estimator=final_pipe,
    param_grid=param_grid,
    scoring='f1',
    cv=5,
    n_jobs=-1
)

grid.fit(X_train,Y_train)

# print('\n\n Best Params', grid.best_params_)
# print('\n\n Best Score', grid.best_score_)

best_estimator = grid.best_estimator_
bestYpred = best_estimator.predict(X_test)
# print(classification_report(Y_test,bestYpred))

cv_df = pd.DataFrame(grid.cv_results_)
cv_df[['params', 'mean_test_score', 'std_test_score']].sort_values(
    by='mean_test_score', ascending=False
).head()
print(cv_df)





''' ------------ ** Theory ----------------------
1Ô∏è‚É£ Why tuning is necessary (even when metrics look fine)

From Day 5 you saw:
	‚Ä¢	Changing C or penalty didn‚Äôt change accuracy much
	‚Ä¢	Everything looked ‚Äústable‚Äù

That‚Äôs exactly the danger zone.

Why?

Because:
	‚Ä¢	One train/test split can be lucky
	‚Ä¢	Another split may behave differently
	‚Ä¢	You don‚Äôt know if your chosen C=1.0 is robust

So we ask a better question:

Which hyperparameters perform well across many data splits?

That‚Äôs cross-validation.

‚∏ª

2Ô∏è‚É£ What cross-validation really is (no buzzwords)

Instead of:
	‚Ä¢	Train once
	‚Ä¢	Test once

We do:
	‚Ä¢	Split data into K folds
	‚Ä¢	Train on K-1 folds
	‚Ä¢	Validate on the remaining fold
	‚Ä¢	Repeat K times
	‚Ä¢	Average the results

This answers:

‚ÄúHow stable is this model across different samples of data?‚Äù

‚∏ª

3Ô∏è‚É£ Why we tune hyperparameters with CV (not the test set)

Very important rule:

The test set is sacred.
You do NOT tune on it.

So:
	‚Ä¢	Training set ‚Üí used for CV
	‚Ä¢	Validation (inside CV) ‚Üí used to pick hyperparameters
	‚Ä¢	Test set ‚Üí used once at the end

GridSearchCV enforces this discipline.

‚∏ª

4Ô∏è‚É£ GridSearchCV ‚Äî what it really does

GridSearchCV:
	‚Ä¢	Takes a pipeline
	‚Ä¢	Takes a parameter grid
	‚Ä¢	Performs cross-validation for every combination
	‚Ä¢	Picks the one with the best mean CV score

It is NOT:
	‚Ä¢	‚ÄúTry everything and pick the highest test accuracy‚Äù

It IS:
	‚Ä¢	‚ÄúFind the most stable configuration‚Äù

‚∏ª

5Ô∏è‚É£ Define the parameter grid (logistic regression‚Äìspecific)

We‚Äôll tune:
	‚Ä¢	C
	‚Ä¢	penalty
	‚Ä¢	solver
'''